{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attack sequence prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4AWqs2P5yW_",
        "outputId": "cf7c334b-3d6a-4ffb-ab80-169e7e945673"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 16 12:54:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    60W / 149W |    263MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ElJXZIy4S8"
      },
      "source": [
        "# Experiment 2\n",
        "<font size=5>We'll use an encoder-decoder architecture which is good for seq2seq models. Some portion of our sequences will be used for encoder input and rest for decoder inputs.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS8NTwd09rKN"
      },
      "source": [
        "We'll first use teacher forcing for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE3dHxtZuHsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de32cba-7d9a-4658-e501-0031009fddc4"
      },
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4pS5oi52lTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977c1901-69a8-4fdd-eacd-b70451e9fd9d"
      },
      "source": [
        "%cd /content/drive/MyDrive/Thesis/Attack\\ Step\\ Prediction/Implementation\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Thesis/Attack Step Prediction/Implementation\n",
            "APTGen_json\t __pycache__  s2s_stacked    test_data_sequence.txt\n",
            "aptgen_utils.py  s2s_normal   sequences.csv  unique_steps.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR8m8QbR-DIE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "\n",
        "import aptgen_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLsO9090-EW-",
        "outputId": "a5ecc0a9-177e-42d2-9499-73f78011f6db"
      },
      "source": [
        "# to reflect the changes, the module needs to be reloaded\n",
        "import importlib\n",
        "importlib.reload(aptgen_utils)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'aptgen_utils' from '/content/drive/MyDrive/Thesis/Attack Step Prediction/Implementation/aptgen_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKUB_hODM67"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OErpNauQ-IdS",
        "outputId": "199225c1-6d49-49bb-c544-5940302e342a"
      },
      "source": [
        "# get attack sequences as list of lists\n",
        "data_text = aptgen_utils.get_data_text()\n",
        "print(data_text[0])\n",
        "len(data_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['collection TA0009 Email_Collection T1114', 'credential_access TA0006 Credential_Dumping T1003', 'discovery TA0007 System_Information_Discovery T1082', 'collection TA0009 Email_Collection T1114', 'defense_evasion TA0005 File_Deletion T1107', 'persistence TA0003 Scheduled_Task T1053', 'discovery TA0007 System_Information_Discovery T1082', 'collection TA0009 Email_Collection T1114']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ubeqncTBaL8"
      },
      "source": [
        "We'll strip all spaces so that `Tokenizer()` can build *word_index* later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_kJgvrq_uxM",
        "outputId": "16e268c2-3f8b-462b-c42e-c5d5759ed9bd"
      },
      "source": [
        "# strip all spaces from data_text\n",
        "data_text = [[''.join(''.join(step).split()) for step in data] for data in data_text]\n",
        "print(data_text[0])\n",
        "len(data_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['collectionTA0009Email_CollectionT1114', 'credential_accessTA0006Credential_DumpingT1003', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114', 'defense_evasionTA0005File_DeletionT1107', 'persistenceTA0003Scheduled_TaskT1053', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMXXwXwaBq39"
      },
      "source": [
        "Now, let's find the frequency of different sequence length to determine how much timestep of input should go to the encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "rL08OFC0CP_C",
        "outputId": "fbec6958-4483-4f89-f3d0-6dc41ad07c92"
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get length of different sequences in a list\n",
        "sequence_lengths = [len(sequence) for sequence in data_text]\n",
        "print(sequence_lengths)\n",
        "\n",
        "counter = Counter(sequence_lengths)\n",
        "print(\"Frequncy of sequence lengths:\", counter)\n",
        "print(\"Number of unique sequence lengths:\", len(counter.keys()))\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "x = [key for key in counter.keys()]\n",
        "y = [val for val in counter.values()]\n",
        "\n",
        "plt.xticks(x)\n",
        "plt.stem(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 5, 6, 6, 8, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 18, 18, 7, 7, 7, 8, 8, 9, 9, 9, 9, 9, 10, 11, 11, 12, 12, 12, 12, 12, 13, 13, 6, 8, 9, 10, 13, 17, 18, 19, 19, 21, 21, 21, 22, 23, 24, 25, 25, 25, 25, 26, 16, 16, 17, 19, 19, 22, 22, 22, 24, 26, 27, 27, 28, 29, 29, 30, 31, 32, 34, 35, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 12, 12, 12, 12, 12, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 18, 18, 18, 20, 6, 6, 7, 7, 8, 9, 9, 9, 10, 10, 10, 11, 11, 12, 13, 14, 14, 16, 18, 21, 4, 6, 7, 8, 11, 13, 15, 17, 17, 18, 18, 19, 20, 22, 22, 24, 25, 25, 25, 25, 13, 14, 16, 17, 17, 18, 20, 20, 24, 25, 25, 26, 26, 26, 27, 27, 28, 28, 29, 30, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 6, 7, 7, 9, 11, 12, 12, 13, 13, 13, 14, 14, 14, 15, 16, 16, 18, 18, 19, 9, 9, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 18, 14, 14, 15, 16, 17, 18, 18, 19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 22, 23, 11, 17, 18, 19, 21, 21, 21, 23, 24, 24, 25, 27, 27, 27, 28, 28, 28, 29, 29, 30, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 6, 6, 7, 7, 11, 11, 11, 13, 14, 14, 15, 15, 15, 15, 16, 17, 17, 17, 8, 11, 12, 12, 13, 13, 13, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 17, 18, 18, 19, 22, 22, 23, 25, 25, 26, 27, 27, 28, 29, 29, 30, 30, 30, 31, 32, 32, 32, 32, 16, 20, 21, 22, 24, 25, 26, 28, 28, 29, 30, 31, 32, 32, 33, 33, 33, 33, 34, 34, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 9, 9, 9, 9, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 10, 10, 11, 12, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9, 10, 11, 12, 13, 9, 10, 12, 13, 13, 14, 14, 14, 17, 18, 18, 18, 19, 20, 21, 21, 22, 22, 22, 22, 6, 7, 8, 10, 12, 14, 14, 14, 14, 14, 14, 16, 16, 17, 17, 18, 19, 20, 21, 22, 6, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 11, 9, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 18, 19, 19, 19, 6, 6, 8, 8, 9, 9, 9, 11, 11, 12, 12, 13, 13, 13, 13, 15, 16, 16, 16, 16, 8, 12, 12, 12, 14, 14, 14, 14, 14, 16, 17, 19, 20, 21, 21, 22, 22, 23, 23, 24, 7, 8, 10, 11, 11, 12, 12, 13, 14, 15, 15, 16, 16, 17, 18, 18, 18, 21, 21, 22, 5, 5, 5, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 11, 11, 11, 11, 11, 12, 12, 6, 6, 6, 7, 8, 8, 8, 9, 10, 10, 10, 11, 12, 13, 13, 13, 14, 14, 14, 15, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 7, 8, 8, 9, 10, 10, 11, 12, 12, 13, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 8, 8, 9, 12, 12, 12, 13, 14, 14, 14, 15, 15, 15, 15, 16, 16, 17, 17, 17, 18, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 8, 8, 9, 9, 10, 10, 10, 11, 12, 12, 13, 13, 14, 15, 15, 12, 12, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 18, 18, 18, 19, 19, 19, 22, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 6, 9, 9, 12, 12, 12, 13, 13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 17]\n",
            "Frequncy of sequence lengths: Counter({8: 117, 6: 62, 14: 56, 12: 49, 15: 45, 9: 44, 16: 42, 7: 39, 13: 37, 11: 32, 18: 30, 5: 29, 10: 29, 17: 28, 22: 23, 19: 19, 21: 18, 25: 14, 20: 10, 27: 9, 28: 9, 24: 8, 29: 8, 26: 7, 30: 7, 32: 7, 23: 6, 4: 5, 33: 4, 31: 3, 34: 3, 35: 1})\n",
            "Number of unique sequence lengths: 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2A5bHmyEc4N",
        "outputId": "c9268200-468e-4d9d-fadd-e4caddb868e5"
      },
      "source": [
        "counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({4: 5,\n",
              "         5: 29,\n",
              "         6: 62,\n",
              "         7: 39,\n",
              "         8: 117,\n",
              "         9: 44,\n",
              "         10: 29,\n",
              "         11: 32,\n",
              "         12: 49,\n",
              "         13: 37,\n",
              "         14: 56,\n",
              "         15: 45,\n",
              "         16: 42,\n",
              "         17: 28,\n",
              "         18: 30,\n",
              "         19: 19,\n",
              "         20: 10,\n",
              "         21: 18,\n",
              "         22: 23,\n",
              "         23: 6,\n",
              "         24: 8,\n",
              "         25: 14,\n",
              "         26: 7,\n",
              "         27: 9,\n",
              "         28: 9,\n",
              "         29: 8,\n",
              "         30: 7,\n",
              "         31: 3,\n",
              "         32: 7,\n",
              "         33: 4,\n",
              "         34: 3,\n",
              "         35: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n74NwqEvGytb"
      },
      "source": [
        "As the shortest length(4) occurs only 4 times and long lengths(33, 34, 35) occur respectively 4, 3 & 1 times only, so we'll not consider those sequences in our experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzWb_UAmH1jL"
      },
      "source": [
        "def remove_sequence(data_text, lengths_to_remove):\n",
        "    lengths_to_remove = lengths_to_remove\n",
        "    truncated_data_text = [sequence for sequence in data_text if len(sequence) not in lengths_to_remove]\n",
        "    print(\"First sequence after truncating:\", truncated_data_text[0])\n",
        "    print(\"Number of sequence after truncating:\", len(truncated_data_text))\n",
        "\n",
        "    return truncated_data_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIU8Nio5Kcdh",
        "outputId": "f6cf33b7-5da0-4210-88de-2ee0cb83d063"
      },
      "source": [
        "# remove sequences having length 4, 33, 34, 35\n",
        "lengths_to_remove = [4, 33, 34, 35]\n",
        "truncated_data_text = remove_sequence(data_text, lengths_to_remove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First sequence after truncating: ['collectionTA0009Email_CollectionT1114', 'credential_accessTA0006Credential_DumpingT1003', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114', 'defense_evasionTA0005File_DeletionT1107', 'persistenceTA0003Scheduled_TaskT1053', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114']\n",
            "Number of sequence after truncating: 787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ABxfIENqmwQ"
      },
      "source": [
        "At first, we'll take `4` steps as the *encoder_input* and remaining steps as *decoder_input*. We'll add `<sos>` as the start symbol before all decoder_input. *decoder_target* will be same as decoder_input except they will be shifted one timestep left. We'll add `<eos>` to the right of every decoder_target sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt25hLhwsdCA"
      },
      "source": [
        "# split data into encoder and decoder text\n",
        "def split_encoder_decoder_text(truncated_data_text, split):\n",
        "    split = split\n",
        "    encoder_input_text, decoder_input_text, decoder_target_text = [], [], []\n",
        "\n",
        "    for sequence in truncated_data_text:\n",
        "        encoder_input_text.append(sequence[:split])\n",
        "        decoder_input_text.append([\"<sos>\"] + sequence[split:]) # add <sos> in the beginning of each sequence\n",
        "        decoder_target_text.append(sequence[split:] + [\"<eos>\"]) # add <eos> at the end of each sequence\n",
        "    \n",
        "    print(\"encoder_input_text[0]:\", encoder_input_text[0], \"\\ndecoder_input_text[0]:\", decoder_input_text[0], \"\\ndecoder_target_text[0]:\", decoder_target_text[0], \"\\n\")\n",
        "    print(\"encoder_input_text[69]:\", encoder_input_text[69], \"\\ndecoder_input_text[69]:\", decoder_input_text[69], \"\\ndecoder_target_text[69]:\", decoder_target_text[69], \"\\n\")\n",
        "    print(\"encoder_input_text[169]:\", encoder_input_text[169], \"\\ndecoder_input_text[169]:\", decoder_input_text[169], \"\\ndecoder_target_text[169]:\", decoder_target_text[169], \"\\n\")\n",
        "    print(\"encoder_input_text[650]:\", encoder_input_text[650], \"\\ndecoder_input_text[650]:\", decoder_input_text[650], \"\\ndecoder_target_text[650]:\", decoder_target_text[650], \"\\n\")\n",
        "\n",
        "    return encoder_input_text, decoder_input_text, decoder_target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdY3xmRjxNY5",
        "outputId": "a60161ae-b6ab-4fd7-a1ac-c044a5a484ec"
      },
      "source": [
        "# split each sequence at 4th step\n",
        "split = 4\n",
        "encoder_input_text, decoder_input_text, decoder_target_text = split_encoder_decoder_text(truncated_data_text, split)\n",
        "print(\"Number of samples:\", len(encoder_input_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_input_text[0]: ['collectionTA0009Email_CollectionT1114', 'credential_accessTA0006Credential_DumpingT1003', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114'] \n",
            "decoder_input_text[0]: ['<sos>', 'defense_evasionTA0005File_DeletionT1107', 'persistenceTA0003Scheduled_TaskT1053', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114'] \n",
            "decoder_target_text[0]: ['defense_evasionTA0005File_DeletionT1107', 'persistenceTA0003Scheduled_TaskT1053', 'discoveryTA0007System_Information_DiscoveryT1082', 'collectionTA0009Email_CollectionT1114', '<eos>'] \n",
            "\n",
            "encoder_input_text[69]: ['discoveryTA0007System_Network_Configuration_DiscoveryT1016', 'collectionTA0009Data_StagedT1074', 'persistenceTA0003Scheduled_TaskT1053', 'credential_accessTA0006Credentials_in_RegistryT1214'] \n",
            "decoder_input_text[69]: ['<sos>', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'collectionTA0009Data_StagedT1074', 'collectionTA0009Email_CollectionT1114', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'defense_evasionTA0005File_DeletionT1107', 'discoveryTA0007Remote_System_DiscoveryT1018', 'defense_evasionTA0005File_DeletionT1107', 'credential_accessTA0006Credentials_in_RegistryT1214', 'discoveryTA0007Account_DiscoveryT1087', 'collectionTA0009Email_CollectionT1114', 'credential_accessTA0006Credentials_in_RegistryT1214', 'collectionTA0009Data_StagedT1074', 'defense_evasionTA0005File_DeletionT1107', 'discoveryTA0007File_and_Directory_DiscoveryT1083', 'discoveryTA0007System_Information_DiscoveryT1082', 'lateral_movementTA0008Remote_File_CopyT1105', 'defense_evasionTA0005File_DeletionT1107'] \n",
            "decoder_target_text[69]: ['exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'collectionTA0009Data_StagedT1074', 'collectionTA0009Email_CollectionT1114', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'defense_evasionTA0005File_DeletionT1107', 'discoveryTA0007Remote_System_DiscoveryT1018', 'defense_evasionTA0005File_DeletionT1107', 'credential_accessTA0006Credentials_in_RegistryT1214', 'discoveryTA0007Account_DiscoveryT1087', 'collectionTA0009Email_CollectionT1114', 'credential_accessTA0006Credentials_in_RegistryT1214', 'collectionTA0009Data_StagedT1074', 'defense_evasionTA0005File_DeletionT1107', 'discoveryTA0007File_and_Directory_DiscoveryT1083', 'discoveryTA0007System_Information_DiscoveryT1082', 'lateral_movementTA0008Remote_File_CopyT1105', 'defense_evasionTA0005File_DeletionT1107', '<eos>'] \n",
            "\n",
            "encoder_input_text[169]: ['persistenceTA0003Component_Object_Model_HijackingT1122', 'discoveryTA0007File_and_Directory_DiscoveryT1083', 'credential_accessTA0006Credential_DumpingT1003', 'discoveryTA0007System_Network_Configuration_DiscoveryT1016'] \n",
            "decoder_input_text[169]: ['<sos>', 'credential_accessTA0006Credential_DumpingT1003', 'collectionTA0009Data_StagedT1074', 'credential_accessTA0006Credential_DumpingT1003', 'defense_evasionTA0005Component_Object_Model_HijackingT1122', 'exfiltrationTA0010Data_EncryptedT1022', 'collectionTA0009Data_StagedT1074', 'discoveryTA0007Security_Software_DiscoveryT1063', 'discoveryTA0007Remote_System_DiscoveryT1018', 'credential_accessTA0006Credential_DumpingT1003', 'credential_accessTA0006Credential_DumpingT1003', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'discoveryTA0007System_Information_DiscoveryT1082', 'lateral_movementTA0008Remote_File_CopyT1105', 'credential_accessTA0006Credential_DumpingT1003', 'persistenceTA0003Component_Object_Model_HijackingT1122', 'discoveryTA0007System_Network_Configuration_DiscoveryT1016', 'credential_accessTA0006Credential_DumpingT1003', 'defense_evasionTA0005File_DeletionT1107'] \n",
            "decoder_target_text[169]: ['credential_accessTA0006Credential_DumpingT1003', 'collectionTA0009Data_StagedT1074', 'credential_accessTA0006Credential_DumpingT1003', 'defense_evasionTA0005Component_Object_Model_HijackingT1122', 'exfiltrationTA0010Data_EncryptedT1022', 'collectionTA0009Data_StagedT1074', 'discoveryTA0007Security_Software_DiscoveryT1063', 'discoveryTA0007Remote_System_DiscoveryT1018', 'credential_accessTA0006Credential_DumpingT1003', 'credential_accessTA0006Credential_DumpingT1003', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'discoveryTA0007System_Information_DiscoveryT1082', 'lateral_movementTA0008Remote_File_CopyT1105', 'credential_accessTA0006Credential_DumpingT1003', 'persistenceTA0003Component_Object_Model_HijackingT1122', 'discoveryTA0007System_Network_Configuration_DiscoveryT1016', 'credential_accessTA0006Credential_DumpingT1003', 'defense_evasionTA0005File_DeletionT1107', '<eos>'] \n",
            "\n",
            "encoder_input_text[650]: ['discoveryTA0007File_and_Directory_DiscoveryT1083', 'persistenceTA0003Logon_ScriptsT1037', 'defense_evasionTA0005Indicator_Removal_on_HostT1070', 'credential_accessTA0006Input_CaptureT1056'] \n",
            "decoder_input_text[650]: ['<sos>', 'discoveryTA0007Password_Policy_DiscoveryT1201', 'defense_evasionTA0005MasqueradingT1036', 'collectionTA0009Data_StagedT1074', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'defense_evasionTA0005Indicator_Removal_on_HostT1070'] \n",
            "decoder_target_text[650]: ['discoveryTA0007Password_Policy_DiscoveryT1201', 'defense_evasionTA0005MasqueradingT1036', 'collectionTA0009Data_StagedT1074', 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 'defense_evasionTA0005Indicator_Removal_on_HostT1070', '<eos>'] \n",
            "\n",
            "Number of samples: 787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUPRn38l0rxL"
      },
      "source": [
        "# Text sequences to integer sequences\n",
        "\n",
        "Now that we've set the encoder and decoder text, lets build the word2idx and idx2word dictionary mapping which will later be used for conversion purpose. We'll also convert the text sequences to integer sequences as deep learning framework can only handle numbers.\n",
        "\n",
        "We'll use the `Tokenizer()` class for above purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUePzTqtveAN"
      },
      "source": [
        "import copy\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def get_tokenizer(text):\n",
        "    tokenizer = Tokenizer(num_words=200, lower=False) # we just give a large enough arbitrary number\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    \n",
        "    # builid word2idx and idx2word dictionary\n",
        "    word2idx = copy.deepcopy(tokenizer.word_index)\n",
        "    idx2word = {v:k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "    return tokenizer, word2idx, idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzJfI7aPoQj"
      },
      "source": [
        "def get_encoder_decoder_indices(encoder_input_text, decoder_input_text, decoder_target_text):\n",
        "    encoder_tokenizer, encoder_word2idx, encoder_idx2word = get_tokenizer(encoder_input_text)\n",
        "    print(\"encoder_word2idx:\", encoder_word2idx)\n",
        "    print(\"encoder_idx2word:\", encoder_idx2word)\n",
        "    encoder_input_indices = encoder_tokenizer.texts_to_sequences(encoder_input_text)\n",
        "\n",
        "\n",
        "    decoder_tokenizer, decoder_word2idx, decoder_idx2word = get_tokenizer(decoder_input_text + decoder_target_text)\n",
        "    print(\"\\ndecoder_word2idx:\", decoder_word2idx)\n",
        "    print(\"decoder_idx2word:\", decoder_idx2word)\n",
        "    decoder_input_indices = decoder_tokenizer.texts_to_sequences(decoder_input_text)\n",
        "    decoder_target_indices = decoder_tokenizer.texts_to_sequences(decoder_target_text)\n",
        "\n",
        "    print(\"\\nencoder_input_indices[0]:\", encoder_input_indices[0], \"\\ndecoder_input_indices[0]:\", decoder_input_indices[0], \"\\ndecoder_target_indices[0]:\", decoder_target_indices[0], \"\\n\")\n",
        "    print(\"encoder_input_indices[69]:\", encoder_input_indices[69], \"\\ndecoder_input_indices[69]:\", decoder_input_indices[69], \"\\ndecoder_target_indices[69]:\", decoder_target_indices[69], \"\\n\")\n",
        "    print(\"encoder_input_indices[169]:\", encoder_input_indices[169], \"\\ndecoder_input_indices[169]:\", decoder_input_indices[169], \"\\ndecoder_target_indices[169]:\", decoder_target_indices[169], \"\\n\")\n",
        "    print(\"encoder_input_indices[650]:\", encoder_input_indices[650], \"\\ndecoder_input_indices[650]:\", decoder_input_indices[650], \"\\ndecoder_target_indices[650]:\", decoder_target_indices[650], \"\\n\")\n",
        "\n",
        "    return encoder_input_indices, decoder_input_indices, decoder_target_indices, encoder_word2idx, encoder_idx2word, decoder_word2idx, decoder_idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsBvvvasV476",
        "outputId": "f795e627-7372-4a40-c35a-96535df719ae"
      },
      "source": [
        "# get input_indices and word conversion dicts\n",
        "encoder_input_indices, decoder_input_indices, decoder_target_indices, encoder_word2idx, encoder_idx2word, decoder_word2idx, decoder_idx2word \\\n",
        "= \\\n",
        "get_encoder_decoder_indices(encoder_input_text, decoder_input_text, decoder_target_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_word2idx: {'credential_accessTA0006Credential_DumpingT1003': 1, 'discoveryTA0007File_and_Directory_DiscoveryT1083': 2, 'discoveryTA0007Remote_System_DiscoveryT1018': 3, 'defense_evasionTA0005Deobfuscate/Decode_Files_or_InformationT1140': 4, 'collectionTA0009Input_CaptureT1056': 5, 'defense_evasionTA0005File_DeletionT1107': 6, 'collectionTA0009Data_StagedT1074': 7, 'credential_accessTA0006Input_CaptureT1056': 8, 'collectionTA0009Email_CollectionT1114': 9, 'persistenceTA0003Scheduled_TaskT1053': 10, 'persistenceTA0003New_ServiceT1050': 11, 'defense_evasionTA0005Indicator_Removal_on_HostT1070': 12, 'credential_accessTA0006Credentials_in_FilesT1081': 13, 'discoveryTA0007System_Time_DiscoveryT1124': 14, 'credential_accessTA0006Credentials_in_RegistryT1214': 15, 'discoveryTA0007Account_DiscoveryT1087': 16, 'lateral_movementTA0008Remote_File_CopyT1105': 17, 'discoveryTA0007System_Network_Configuration_DiscoveryT1016': 18, 'persistenceTA0003Logon_ScriptsT1037': 19, 'discoveryTA0007Query_RegistryT1012': 20, 'persistenceTA0003Winlogon_Helper_DLLT1004': 21, 'discoveryTA0007System_Information_DiscoveryT1082': 22, 'persistenceTA0003Windows_Management_Instrumentation_Event_SubscriptionT1084': 23, 'defense_evasionTA0005MasqueradingT1036': 24, 'discoveryTA0007System_Service_DiscoveryT1007': 25, 'discoveryTA0007Password_Policy_DiscoveryT1201': 26, 'defense_evasionTA0005Image_File_Execution_Options_InjectionT1183': 27, 'persistenceTA0003Component_Object_Model_HijackingT1122': 28, 'persistenceTA0003AppInit_DLLsT1103': 29, 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041': 30, 'persistenceTA0003Image_File_Execution_Options_InjectionT1183': 31, 'persistenceTA0003Accessibility_FeaturesT1015': 32, 'discoveryTA0007Security_Software_DiscoveryT1063': 33, 'defense_evasionTA0005Component_Object_Model_HijackingT1122': 34, 'persistenceTA0003HookingT1179': 35, 'persistenceTA0003Netsh_Helper_DLLT1128': 36, 'lateral_movementTA0008Logon_ScriptsT1037': 37, 'discoveryTA0007Network_Share_DiscoveryT1135': 38, 'persistenceTA0003Security_Support_ProviderT1101': 39, 'credential_accessTA0006HookingT1179': 40, 'persistenceTA0003Registry_Run_Keys_/_Startup_FolderT1060': 41, 'discoveryTA0007Permission_Groups_DiscoveryT1069': 42, 'lateral_movementTA0008Pass_the_HashT1075': 43, 'discoveryTA0007System_Network_Connections_DiscoveryT1049': 44, 'exfiltrationTA0010Data_EncryptedT1022': 45, 'credential_accessTA0006Brute_ForceT1110': 46, 'lateral_movementTA0008Windows_Remote_ManagementT1028': 47, 'lateral_movementTA0008Remote_Desktop_ProtocolT1076': 48, 'discoveryTA0007System_Owner/User_DiscoveryT1033': 49, 'exfiltrationTA0010Data_CompressedT1002': 50}\n",
            "encoder_idx2word: {1: 'credential_accessTA0006Credential_DumpingT1003', 2: 'discoveryTA0007File_and_Directory_DiscoveryT1083', 3: 'discoveryTA0007Remote_System_DiscoveryT1018', 4: 'defense_evasionTA0005Deobfuscate/Decode_Files_or_InformationT1140', 5: 'collectionTA0009Input_CaptureT1056', 6: 'defense_evasionTA0005File_DeletionT1107', 7: 'collectionTA0009Data_StagedT1074', 8: 'credential_accessTA0006Input_CaptureT1056', 9: 'collectionTA0009Email_CollectionT1114', 10: 'persistenceTA0003Scheduled_TaskT1053', 11: 'persistenceTA0003New_ServiceT1050', 12: 'defense_evasionTA0005Indicator_Removal_on_HostT1070', 13: 'credential_accessTA0006Credentials_in_FilesT1081', 14: 'discoveryTA0007System_Time_DiscoveryT1124', 15: 'credential_accessTA0006Credentials_in_RegistryT1214', 16: 'discoveryTA0007Account_DiscoveryT1087', 17: 'lateral_movementTA0008Remote_File_CopyT1105', 18: 'discoveryTA0007System_Network_Configuration_DiscoveryT1016', 19: 'persistenceTA0003Logon_ScriptsT1037', 20: 'discoveryTA0007Query_RegistryT1012', 21: 'persistenceTA0003Winlogon_Helper_DLLT1004', 22: 'discoveryTA0007System_Information_DiscoveryT1082', 23: 'persistenceTA0003Windows_Management_Instrumentation_Event_SubscriptionT1084', 24: 'defense_evasionTA0005MasqueradingT1036', 25: 'discoveryTA0007System_Service_DiscoveryT1007', 26: 'discoveryTA0007Password_Policy_DiscoveryT1201', 27: 'defense_evasionTA0005Image_File_Execution_Options_InjectionT1183', 28: 'persistenceTA0003Component_Object_Model_HijackingT1122', 29: 'persistenceTA0003AppInit_DLLsT1103', 30: 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 31: 'persistenceTA0003Image_File_Execution_Options_InjectionT1183', 32: 'persistenceTA0003Accessibility_FeaturesT1015', 33: 'discoveryTA0007Security_Software_DiscoveryT1063', 34: 'defense_evasionTA0005Component_Object_Model_HijackingT1122', 35: 'persistenceTA0003HookingT1179', 36: 'persistenceTA0003Netsh_Helper_DLLT1128', 37: 'lateral_movementTA0008Logon_ScriptsT1037', 38: 'discoveryTA0007Network_Share_DiscoveryT1135', 39: 'persistenceTA0003Security_Support_ProviderT1101', 40: 'credential_accessTA0006HookingT1179', 41: 'persistenceTA0003Registry_Run_Keys_/_Startup_FolderT1060', 42: 'discoveryTA0007Permission_Groups_DiscoveryT1069', 43: 'lateral_movementTA0008Pass_the_HashT1075', 44: 'discoveryTA0007System_Network_Connections_DiscoveryT1049', 45: 'exfiltrationTA0010Data_EncryptedT1022', 46: 'credential_accessTA0006Brute_ForceT1110', 47: 'lateral_movementTA0008Windows_Remote_ManagementT1028', 48: 'lateral_movementTA0008Remote_Desktop_ProtocolT1076', 49: 'discoveryTA0007System_Owner/User_DiscoveryT1033', 50: 'exfiltrationTA0010Data_CompressedT1002'}\n",
            "\n",
            "decoder_word2idx: {'collectionTA0009Data_StagedT1074': 1, 'credential_accessTA0006Credential_DumpingT1003': 2, 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041': 3, 'defense_evasionTA0005File_DeletionT1107': 4, 'discoveryTA0007File_and_Directory_DiscoveryT1083': 5, 'discoveryTA0007Remote_System_DiscoveryT1018': 6, 'lateral_movementTA0008Remote_File_CopyT1105': 7, '<sos>': 8, '<eos>': 9, 'defense_evasionTA0005Deobfuscate/Decode_Files_or_InformationT1140': 10, 'collectionTA0009Input_CaptureT1056': 11, 'collectionTA0009Email_CollectionT1114': 12, 'defense_evasionTA0005Indicator_Removal_on_HostT1070': 13, 'discoveryTA0007System_Time_DiscoveryT1124': 14, 'credential_accessTA0006Input_CaptureT1056': 15, 'credential_accessTA0006Credentials_in_FilesT1081': 16, 'lateral_movementTA0008Remote_Desktop_ProtocolT1076': 17, 'credential_accessTA0006Credentials_in_RegistryT1214': 18, 'persistenceTA0003New_ServiceT1050': 19, 'discoveryTA0007System_Network_Configuration_DiscoveryT1016': 20, 'discoveryTA0007Account_DiscoveryT1087': 21, 'persistenceTA0003Scheduled_TaskT1053': 22, 'discoveryTA0007Network_Share_DiscoveryT1135': 23, 'defense_evasionTA0005Image_File_Execution_Options_InjectionT1183': 24, 'discoveryTA0007System_Service_DiscoveryT1007': 25, 'discoveryTA0007System_Information_DiscoveryT1082': 26, 'discoveryTA0007Query_RegistryT1012': 27, 'lateral_movementTA0008Pass_the_HashT1075': 28, 'defense_evasionTA0005Component_Object_Model_HijackingT1122': 29, 'credential_accessTA0006Brute_ForceT1110': 30, 'exfiltrationTA0010Data_CompressedT1002': 31, 'lateral_movementTA0008Windows_Remote_ManagementT1028': 32, 'exfiltrationTA0010Data_EncryptedT1022': 33, 'discoveryTA0007Security_Software_DiscoveryT1063': 34, 'lateral_movementTA0008Logon_ScriptsT1037': 35, 'discoveryTA0007System_Network_Connections_DiscoveryT1049': 36, 'defense_evasionTA0005MasqueradingT1036': 37, 'persistenceTA0003Registry_Run_Keys_/_Startup_FolderT1060': 38, 'discoveryTA0007Password_Policy_DiscoveryT1201': 39, 'discoveryTA0007System_Owner/User_DiscoveryT1033': 40, 'persistenceTA0003Component_Object_Model_HijackingT1122': 41, 'discoveryTA0007Permission_Groups_DiscoveryT1069': 42, 'persistenceTA0003Winlogon_Helper_DLLT1004': 43, 'credential_accessTA0006HookingT1179': 44, 'persistenceTA0003Windows_Management_Instrumentation_Event_SubscriptionT1084': 45, 'persistenceTA0003Image_File_Execution_Options_InjectionT1183': 46, 'persistenceTA0003AppInit_DLLsT1103': 47, 'persistenceTA0003Security_Support_ProviderT1101': 48, 'persistenceTA0003Logon_ScriptsT1037': 49, 'persistenceTA0003HookingT1179': 50, 'persistenceTA0003Netsh_Helper_DLLT1128': 51, 'persistenceTA0003Accessibility_FeaturesT1015': 52}\n",
            "decoder_idx2word: {1: 'collectionTA0009Data_StagedT1074', 2: 'credential_accessTA0006Credential_DumpingT1003', 3: 'exfiltrationTA0010Exfiltration_Over_Command_and_Control_ChannelT1041', 4: 'defense_evasionTA0005File_DeletionT1107', 5: 'discoveryTA0007File_and_Directory_DiscoveryT1083', 6: 'discoveryTA0007Remote_System_DiscoveryT1018', 7: 'lateral_movementTA0008Remote_File_CopyT1105', 8: '<sos>', 9: '<eos>', 10: 'defense_evasionTA0005Deobfuscate/Decode_Files_or_InformationT1140', 11: 'collectionTA0009Input_CaptureT1056', 12: 'collectionTA0009Email_CollectionT1114', 13: 'defense_evasionTA0005Indicator_Removal_on_HostT1070', 14: 'discoveryTA0007System_Time_DiscoveryT1124', 15: 'credential_accessTA0006Input_CaptureT1056', 16: 'credential_accessTA0006Credentials_in_FilesT1081', 17: 'lateral_movementTA0008Remote_Desktop_ProtocolT1076', 18: 'credential_accessTA0006Credentials_in_RegistryT1214', 19: 'persistenceTA0003New_ServiceT1050', 20: 'discoveryTA0007System_Network_Configuration_DiscoveryT1016', 21: 'discoveryTA0007Account_DiscoveryT1087', 22: 'persistenceTA0003Scheduled_TaskT1053', 23: 'discoveryTA0007Network_Share_DiscoveryT1135', 24: 'defense_evasionTA0005Image_File_Execution_Options_InjectionT1183', 25: 'discoveryTA0007System_Service_DiscoveryT1007', 26: 'discoveryTA0007System_Information_DiscoveryT1082', 27: 'discoveryTA0007Query_RegistryT1012', 28: 'lateral_movementTA0008Pass_the_HashT1075', 29: 'defense_evasionTA0005Component_Object_Model_HijackingT1122', 30: 'credential_accessTA0006Brute_ForceT1110', 31: 'exfiltrationTA0010Data_CompressedT1002', 32: 'lateral_movementTA0008Windows_Remote_ManagementT1028', 33: 'exfiltrationTA0010Data_EncryptedT1022', 34: 'discoveryTA0007Security_Software_DiscoveryT1063', 35: 'lateral_movementTA0008Logon_ScriptsT1037', 36: 'discoveryTA0007System_Network_Connections_DiscoveryT1049', 37: 'defense_evasionTA0005MasqueradingT1036', 38: 'persistenceTA0003Registry_Run_Keys_/_Startup_FolderT1060', 39: 'discoveryTA0007Password_Policy_DiscoveryT1201', 40: 'discoveryTA0007System_Owner/User_DiscoveryT1033', 41: 'persistenceTA0003Component_Object_Model_HijackingT1122', 42: 'discoveryTA0007Permission_Groups_DiscoveryT1069', 43: 'persistenceTA0003Winlogon_Helper_DLLT1004', 44: 'credential_accessTA0006HookingT1179', 45: 'persistenceTA0003Windows_Management_Instrumentation_Event_SubscriptionT1084', 46: 'persistenceTA0003Image_File_Execution_Options_InjectionT1183', 47: 'persistenceTA0003AppInit_DLLsT1103', 48: 'persistenceTA0003Security_Support_ProviderT1101', 49: 'persistenceTA0003Logon_ScriptsT1037', 50: 'persistenceTA0003HookingT1179', 51: 'persistenceTA0003Netsh_Helper_DLLT1128', 52: 'persistenceTA0003Accessibility_FeaturesT1015'}\n",
            "\n",
            "encoder_input_indices[0]: [9, 1, 22, 9] \n",
            "decoder_input_indices[0]: [8, 4, 22, 26, 12] \n",
            "decoder_target_indices[0]: [4, 22, 26, 12, 9] \n",
            "\n",
            "encoder_input_indices[69]: [18, 7, 10, 15] \n",
            "decoder_input_indices[69]: [8, 3, 1, 12, 3, 4, 6, 4, 18, 21, 12, 18, 1, 4, 5, 26, 7, 4] \n",
            "decoder_target_indices[69]: [3, 1, 12, 3, 4, 6, 4, 18, 21, 12, 18, 1, 4, 5, 26, 7, 4, 9] \n",
            "\n",
            "encoder_input_indices[169]: [28, 2, 1, 18] \n",
            "decoder_input_indices[169]: [8, 2, 1, 2, 29, 33, 1, 34, 6, 2, 2, 3, 26, 7, 2, 41, 20, 2, 4] \n",
            "decoder_target_indices[169]: [2, 1, 2, 29, 33, 1, 34, 6, 2, 2, 3, 26, 7, 2, 41, 20, 2, 4, 9] \n",
            "\n",
            "encoder_input_indices[650]: [2, 19, 12, 8] \n",
            "decoder_input_indices[650]: [8, 39, 37, 1, 3, 13] \n",
            "decoder_target_indices[650]: [39, 37, 1, 3, 13, 9] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4aIqHpEXGQf"
      },
      "source": [
        "# Padding, Split & One-hot \n",
        "\n",
        "Encoder inputs all have the same length so we dont need any padding there. But decoder inputs are of variable length and we need padding.\n",
        "\n",
        "Lets find out the maximum sequence length in both encoder and decoder inputs.\n",
        "\n",
        "Note: <font color=\"yellow\">decoder_input and decoder_target both have same sequence length.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYafpFOSSy6U",
        "outputId": "01c95f8b-df2f-40d8-960b-154d64c31fbd"
      },
      "source": [
        "max_encoder_seq_length = max([len(sequence) for sequence in encoder_input_indices])\n",
        "max_decoder_seq_length = max([len(sequence) for sequence in decoder_input_indices])\n",
        "\n",
        "print(\"Max sequence length for encoder:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for decoder:\", max_decoder_seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sequence length for encoder: 4\n",
            "Max sequence length for decoder: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVVs9bozYLVU"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def get_padded_inputs(encoder_input_indices, decoder_input_indices, decoder_target_indices):\n",
        "    \n",
        "    padded_decoder_input = pad_sequences(decoder_input_indices, maxlen=max_decoder_seq_length, dtype='int32', padding='post')\n",
        "    padded_decoder_target = pad_sequences(decoder_target_indices, maxlen=max_decoder_seq_length, dtype='int32', padding='post')\n",
        "    print(\"\\npadded_decoder_input[0]\", padded_decoder_input[0])\n",
        "    print(\"padded_decoder_target[0]\", padded_decoder_target[0])\n",
        "    print(\"padded_decoder_input[69]\", padded_decoder_input[69])\n",
        "    print(\"padded_decoder_target[69\", padded_decoder_target[69])\n",
        "\n",
        "    return padded_decoder_input, padded_decoder_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnQdY8YTZfIP",
        "outputId": "9c79eb91-f315-4abc-f6f4-0b4e378a0d02"
      },
      "source": [
        "padded_decoder_input, padded_decoder_target = get_padded_inputs(encoder_input_indices, decoder_input_indices, decoder_target_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "padded_decoder_input[0] [ 8  4 22 26 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0]\n",
            "padded_decoder_target[0] [ 4 22 26 12  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0]\n",
            "padded_decoder_input[69] [ 8  3  1 12  3  4  6  4 18 21 12 18  1  4  5 26  7  4  0  0  0  0  0  0\n",
            "  0  0  0  0  0]\n",
            "padded_decoder_target[69 [ 3  1 12  3  4  6  4 18 21 12 18  1  4  5 26  7  4  9  0  0  0  0  0  0\n",
            "  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPM5p42EyTPM"
      },
      "source": [
        "Before further processing, lets divide our data into train and test set. we'll randomly take 750 sequences as training data and rest as testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai7T69Q_ya7z"
      },
      "source": [
        "def get_train_test_data(num_train_sample, encoder_input_indices, padded_decoder_input, padded_decoder_target):\n",
        "    num_total_sample = len(encoder_input_indices)\n",
        "    random.seed(69)\n",
        "    random_indices = list(range(num_total_sample))\n",
        "    random.shuffle(random_indices)\n",
        "    # print(random_indices)\n",
        "    encoder_train_input = [encoder_input_indices[i] for i in random_indices[:num_train_sample]]\n",
        "    decoder_train_input = [padded_decoder_input[i] for i in random_indices[:num_train_sample]]\n",
        "    decoder_train_target = [padded_decoder_target[i] for i in random_indices[:num_train_sample]]\n",
        "\n",
        "    encoder_test_input = [encoder_input_indices[i] for i in random_indices[num_train_sample:]]\n",
        "    decoder_test_input = [padded_decoder_input[i] for i in random_indices[num_train_sample:]]\n",
        "    decoder_test_target = [padded_decoder_target[i] for i in random_indices[num_train_sample:]]\n",
        "\n",
        "    print(\"Number of training samples:\", len(encoder_train_input))\n",
        "    print(\"Number of testing samples:\", len(encoder_test_input))\n",
        "\n",
        "    print(\"\\nencoder_train_input[0]:\", encoder_train_input[0], \"\\ndecoder_train_input[0]:\", decoder_train_input[0], \"\\ndecoder_train_target[0]:\", decoder_train_target[0], \"\\n\")\n",
        "    print(\"encoder_train_input[69]:\", encoder_train_input[69], \"\\ndecoder_train_input[69]:\", decoder_train_input[69], \"\\ndecoder_train_target[69]:\", decoder_train_target[69], \"\\n\")\n",
        "    print(\"encoder_test_input[0]:\", encoder_test_input[0], \"\\ndecoder_test_input[0]:\", decoder_test_input[0], \"\\ndecoder_test_target[0]:\", decoder_test_target[0], \"\\n\")\n",
        "    print(\"encoder_test_input[15]:\", encoder_test_input[15], \"\\ndecoder_test_input[15]:\", decoder_test_input[15], \"\\ndecoder_test_target[15]:\", decoder_test_target[15], \"\\n\")\n",
        "\n",
        "    return encoder_train_input, decoder_train_input, decoder_train_target, encoder_test_input, decoder_test_input, decoder_test_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsIEfVf93_E1",
        "outputId": "1824977c-58fc-47d5-9922-1b3875968a99"
      },
      "source": [
        "# split the data into train and test set\n",
        "num_train_sample = 750\n",
        "encoder_train_input, decoder_train_input, decoder_train_target, encoder_test_input, decoder_test_input, decoder_test_target \\\n",
        "= \\\n",
        "get_train_test_data(num_train_sample, encoder_input_indices, padded_decoder_input, padded_decoder_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 750\n",
            "Number of testing samples: 37\n",
            "\n",
            "encoder_train_input[0]: [9, 7, 7, 30] \n",
            "decoder_train_input[0]: [ 8 26  2  5  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "decoder_train_target[0]: [26  2  5  4  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "\n",
            "encoder_train_input[69]: [2, 40, 10, 3] \n",
            "decoder_train_input[69]: [ 8  1 31 31 39  1  6  3 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "decoder_train_target[69]: [ 1 31 31 39  1  6  3 13  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "\n",
            "encoder_test_input[0]: [7, 45, 22, 30] \n",
            "decoder_test_input[0]: [ 8 41  6 10 40  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "decoder_test_target[0]: [41  6 10 40  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0] \n",
            "\n",
            "encoder_test_input[15]: [19, 4, 4, 13] \n",
            "decoder_test_input[15]: [8 5 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
            "decoder_test_target[15]: [5 1 3 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAds_3UGUqhY"
      },
      "source": [
        "Now, we'll find the number of unique tokens in the encoder and decoder input text. This will determine the <font size=5> *vocab_size* </font> and <font size=5>*dimension of one-hot encoding*</font>.\n",
        "\n",
        "The *max value* from the word2idx dict is basically the number of unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fduSU8fIKTJJ",
        "outputId": "d91e97d8-d3d3-403c-84e3-3e630cd0e930"
      },
      "source": [
        "num_encoder_tokens = max(encoder_word2idx.values())\n",
        "num_decoder_tokens = max(decoder_word2idx.values())\n",
        "\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique input tokens: 50\n",
            "Number of unique output tokens: 52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtE2N-GdgrK"
      },
      "source": [
        "Now that our data preprocessing is almost finished, lets give the final touch by converting all the encoder and decoder inputs to <font size=5>*onehot vectors*</font>. For now, we'll only convert the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_E0omZxd0HC"
      },
      "source": [
        "def convert_to_onehot(encoder_train_input, decoder_train_input, decoder_train_target):    \n",
        "    encoder_train_input_oh = tf.one_hot(encoder_train_input, num_encoder_tokens+1, dtype='int32').numpy() # +1 for 0s (were added for padding)\n",
        "    decoder_train_input_oh = tf.one_hot(decoder_train_input, num_decoder_tokens+1, dtype='int32').numpy()\n",
        "    decoder_train_target_oh = tf.one_hot(decoder_train_target, num_decoder_tokens+1, dtype='int32').numpy()\n",
        "\n",
        "    return encoder_train_input_oh, decoder_train_input_oh, decoder_train_target_oh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyMRFnWtZ32l",
        "outputId": "41261015-0891-4975-ad83-ed73041fdcee"
      },
      "source": [
        "encoder_train_input_oh, decoder_train_input_oh, decoder_train_target_oh \\\n",
        "= \\\n",
        "convert_to_onehot(encoder_train_input, decoder_train_input, decoder_train_target)\n",
        "\n",
        "print(\"encoder_train_input_oh shape:\", encoder_train_input_oh.shape)\n",
        "print(\"decoder_train_input_oh shape:\", decoder_train_input_oh.shape)\n",
        "print(\"decoder_train_target_oh shape:\", decoder_train_target_oh.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_train_input_oh shape: (750, 4, 51)\n",
            "decoder_train_input_oh shape: (750, 29, 53)\n",
            "decoder_train_target_oh shape: (750, 29, 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amLuXKoeJsMz",
        "outputId": "ea56ba27-10f2-4934-83bd-7deb22852d47"
      },
      "source": [
        "encoder_train_input_oh[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0aJkmI4KMB0"
      },
      "source": [
        "# Defining the training model\n",
        "\n",
        "Finally, we are done with data preprocessing and we've managed to put all our data in the right shape for feeding into the encoder-decoder architecture. Now, lets define the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZVQVwABAndA"
      },
      "source": [
        "from tensorflow.keras import Model, layers, Input\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KANP2aCqKpB9"
      },
      "source": [
        "# K.clear_session()\n",
        "# n_units = 32\n",
        "# max_enc_seq_len = 4\n",
        "# max_dec_seq_len = 29\n",
        "\n",
        "# # define encoder\n",
        "# encoder_inputs = Input(shape=(None, num_encoder_tokens+1), name=\"encoder_input\")\n",
        "# encoder = layers.LSTM(n_units, return_state=True, name=\"encoder_lstm\")\n",
        "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # define decoder\n",
        "# decoder_inputs = Input(shape=(None, num_decoder_tokens+1), name=\"decoder_input\")\n",
        "# masked_decoder_inputs = layers.Masking(mask_value=0, name=\"decoder_masking\")(decoder_inputs)\n",
        "# decoder_lstm = layers.LSTM(n_units, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "# decoder_outputs, _, _ = decoder_lstm(masked_decoder_inputs, initial_state=encoder_states)\n",
        "# decoder_dense = layers.Dense(num_decoder_tokens+1, activation='softmax', name=\"decoder_dense\")\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # define training model\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx1BvTIcE9KQ"
      },
      "source": [
        "# compile\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k4Vpz1bFXv3"
      },
      "source": [
        "# K.clear_session()\n",
        "# # fit\n",
        "# history = model.fit( \\\n",
        "#             [encoder_train_input_oh, decoder_train_input_oh], decoder_train_target_oh, \\\n",
        "#             batch_size=64, \\\n",
        "#             epochs=150, \\\n",
        "#             validation_split=0.1\n",
        "#         )\n",
        "# model.save(\"s2s_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU2l_cqQs9cN"
      },
      "source": [
        "def plot_history(history):\n",
        "\n",
        "    print(\"Max Train Accuracy\", max(history.history['accuracy']))\n",
        "    print(\"Max Validation Accuracy\", max(history.history['val_accuracy']))\n",
        "    print()\n",
        "\n",
        "    # code for plotting metrics\n",
        "    from matplotlib import pyplot\n",
        "\n",
        "    # plot metrics\n",
        "    pyplot.plot(history.history['loss'])\n",
        "    pyplot.plot(history.history['val_loss'])\n",
        "    pyplot.show()\n",
        "    pyplot.plot(history.history['accuracy'])\n",
        "    pyplot.plot(history.history['val_accuracy'])\n",
        "    pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ03vtkztW2J"
      },
      "source": [
        "# Inference mode\n",
        "\n",
        "Now, lets build the model for inference. Here, the same layers as from the training model will be used. \n",
        "\n",
        "In case of correct prediction, the prediction from previous timestep will be fed into the next timestep as input. In case of wrong prediction, we'll feed the actual step in the original attack sequence as the input. This will help our model to keep track of the actual circumstances and prevent a single wrong prediction to propagate into future timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5E7-S_rvXrX"
      },
      "source": [
        "# from tensorflow.keras import models\n",
        "# model = models.load_model(\"s2s_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcAZbgpIv8Ne"
      },
      "source": [
        "# n_units = 32\n",
        "# # define inference encoder\n",
        "# encoder_inputs = model.input[0]\n",
        "# encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "# encoder_states = [state_h_enc, state_c_enc]\n",
        "# infer_encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# # define inference decoder\n",
        "# decoder_inputs = model.input[1]\n",
        "# decoder_state_input_h = Input(shape=(n_units,), name=\"decoder_state_input_h\")\n",
        "# decoder_state_input_c = Input(shape=(n_units,), name=\"decoder_state_input_c\")\n",
        "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder_lstm = model.layers[3]\n",
        "# decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "# decoder_states = [state_h_dec, state_c_dec]\n",
        "# decoder_dense = model.layers[4]\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# infer_decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states, name=\"normal_model\")\n",
        "# infer_decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ_q5bEd6YcQ"
      },
      "source": [
        "def decode_sequence(infer_encoder_model, infer_decoder_model, input_seq, actual_target_sequence):\n",
        "    input_seq_oh = tf.one_hot(input_seq, num_encoder_tokens+1, dtype='int32').numpy()\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens+1))\n",
        "    # Populate the first character of target sequence with the start character <sos>\n",
        "    target_seq[0, 0, decoder_word2idx['<sos>']] = 1\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = infer_encoder_model.predict(input_seq_oh)\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sequence = []\n",
        "    print(\"Actual Sequence:\", actual_target_sequence)\n",
        "    idx = 0\n",
        "\n",
        "    while not stop_condition:\n",
        "        #print(target_seq)\n",
        "        # output_tokens, h, c = infer_decoder_model.predict([target_seq] + states_value)\n",
        "        to_split = infer_decoder_model.predict([target_seq] + states_value)\n",
        "        output_tokens, states_value = to_split[0], to_split[1:]\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0])\n",
        "        decoded_sequence.append(sampled_token_index)\n",
        "        #sampled_step = decoder_idx2word[sampled_token_index]\n",
        "        \n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character <eos>\n",
        "        if actual_target_sequence[idx] == decoder_word2idx['<eos>']:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens+1))\n",
        "        # print(\"actual:\", actual_target_sequence[idx])\n",
        "        # print(\"predicted:\", sampled_token_index)\n",
        "        if sampled_token_index == actual_target_sequence[idx]:\n",
        "            # print(\"match\")\n",
        "            target_seq[0, 0, sampled_token_index] = 1\n",
        "        else:\n",
        "            target_seq[0, 0, actual_target_sequence[idx]] = 1 # feed in the actual step in case of wrong prediction\n",
        "        \n",
        "        # Update states\n",
        "        # states_value = [h, c]\n",
        "        idx += 1\n",
        "\n",
        "    print(\"Predicted Sequence:\", decoded_sequence)\n",
        "    return decoded_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKWAgDMgKhD6"
      },
      "source": [
        "def predict(infer_encoder_model, infer_decoder_model, encoder_test_input, decoder_test_target): \n",
        "    predicted_sequences = []\n",
        "    correct_steps = []\n",
        "    total_step_count = 0\n",
        "    accurate_prediction_count = 0\n",
        "\n",
        "    num_test_samples = len(encoder_test_input)\n",
        "    for i in range(num_test_samples):\n",
        "        input_seq = encoder_test_input[i:i+1]\n",
        "        decoded_sequence = decode_sequence(infer_encoder_model, infer_decoder_model, input_seq, decoder_test_target[i])\n",
        "        correct_steps = [i for i, j in zip(decoder_test_target[i], decoded_sequence) if i == j]\n",
        "        curr_acc_pred_cnt = len(correct_steps)\n",
        "        accurate_prediction_count += curr_acc_pred_cnt\n",
        "        total_step_count += len(decoded_sequence)\n",
        "        print(curr_acc_pred_cnt, \" step(s) correctly predicted\")\n",
        "\n",
        "        predicted_sequences.append(decoded_sequence)\n",
        "        print()\n",
        "\n",
        "    print(\"Total Predicted Steps:\", total_step_count)\n",
        "    print(\"Total Accurate Prediction:\", accurate_prediction_count)\n",
        "\n",
        "    return predicted_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naV_v3cQSgRH"
      },
      "source": [
        "# predicted_sequences = predict(infer_encoder_model, infer_decoder_model, encoder_test_input, decoder_test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNh8-iKVMjpe"
      },
      "source": [
        "# from itertools import chain\n",
        "# print(\"unique steps in actual data:\", len(np.unique(decoder_test_target)))\n",
        "# print(\"unique steps in predicted data:\", len(Counter(chain(*predicted_sequences))))\n",
        "# print((Counter(chain(*decoder_test_target))))\n",
        "# print(Counter(chain(*predicted_sequences)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFJWFdd2tFKW"
      },
      "source": [
        "Now, lets use stacked LSTM's to see how they perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPYZKSZ_tMeq"
      },
      "source": [
        "def get_stacked_model(n_units):\n",
        "\n",
        "    K.clear_session()\n",
        "    # credit: https://stackoverflow.com/a/56693548/7737870\n",
        "    # latent_dims is an array which defines the depth of the encoder/decoder, as well as how large\n",
        "    # the layers should be. So an array of sizes [a,b,c]  would produce a depth-3 encoder and decoder\n",
        "    # with layer sizes equal to [c,b,a] and [a,b,c] respectively.\n",
        "    encoder_inputs = Input(shape=(None, num_encoder_tokens+1), name=\"encoder_input\")\n",
        "    encoder_outputs = encoder_inputs\n",
        "    encoder_states = []\n",
        "    for j in range(len(n_units))[::-1]:\n",
        "        encoder_outputs, h, c = layers.LSTM(n_units[j], return_state=True, return_sequences=bool(j), name=f\"encoder_lstm_{len(n_units) - j}\")(encoder_outputs)\n",
        "        encoder_states += [h, c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, num_decoder_tokens+1), name=\"decoder_input\")\n",
        "    masked_decoder_inputs = layers.Masking(mask_value=0, name=\"decoder_masking\")(decoder_inputs)\n",
        "    decoder_outputs = masked_decoder_inputs\n",
        "    output_layers = []\n",
        "    for j in range(len(n_units)):\n",
        "        output_layers.append( \\\n",
        "            layers.LSTM(n_units[len(n_units) - j - 1], return_sequences=True, return_state=True, name=f\"decoder_lstm_{j+1}\") \\\n",
        "        )\n",
        "        decoder_outputs, dh, dc = output_layers[-1](decoder_outputs, initial_state=encoder_states[2*j:2*(j+1)])\n",
        "\n",
        "    decoder_dense = layers.Dense(num_decoder_tokens+1, activation='softmax', name=\"decoder_dense\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # define training model\n",
        "    stacked_model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"stacked_model\")\n",
        "\n",
        "    return stacked_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Tx8gENDOYL"
      },
      "source": [
        "# n_units = [32, 32, 32]\n",
        "# stacked_model = get_stacked_model(n_units)\n",
        "# stacked_model.summary()\n",
        "# # compile\n",
        "# stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AgMOoH6DgOU"
      },
      "source": [
        "# K.clear_session()\n",
        "# # fit\n",
        "# history = stacked_model.fit( \\\n",
        "#             [encoder_train_input_oh, decoder_train_input_oh], decoder_train_target_oh, \\\n",
        "#             batch_size=64, \\\n",
        "#             epochs=150, \\\n",
        "#             validation_split=0.1\n",
        "#         )\n",
        "# stacked_model.save(\"s2s_stacked\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evK09Au4EBw6"
      },
      "source": [
        "# plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqXJ4FpaFIzz"
      },
      "source": [
        "# from tensorflow.keras import models\n",
        "# model = models.load_model(\"s2s_stacked\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "wtvS7UYcFvn7"
      },
      "source": [
        "# # define inference encoder\n",
        "# encoder_inputs = model.input[0]\n",
        "# encoder_states = []\n",
        "# for i in range(1, len(n_units)+1):\n",
        "#     encoder_states += model.get_layer(f'encoder_lstm_{i}').output[1:]\n",
        "# infer_encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "# # define inference decoder\n",
        "# decoder_inputs = model.input[1]\n",
        "# decoder_outputs = decoder_inputs\n",
        "# decoder_states_inputs = []\n",
        "# decoder_states = []\n",
        "# for j in range(len(n_units))[::-1]:\n",
        "#     current_state_inputs = [Input(shape=(n_units[j],)) for _ in range(2)]\n",
        "#     temp = model.get_layer(f'decoder_lstm_{len(n_units)-j}')(decoder_outputs, initial_state=current_state_inputs)\n",
        "#     decoder_outputs, curr_states = temp[0], temp[1:]\n",
        "\n",
        "#     decoder_states += curr_states\n",
        "#     decoder_states_inputs += current_state_inputs\n",
        "\n",
        "# decoder_dense = model.get_layer('decoder_dense')\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# infer_decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "# infer_decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEHn-hVDHnwc"
      },
      "source": [
        "# predicted_sequences = predict(infer_encoder_model, infer_decoder_model, encoder_test_input, decoder_test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtZxlsnYMFsu"
      },
      "source": [
        "# from itertools import chain\n",
        "# print(\"unique steps in actual data:\", len(np.unique(decoder_test_target)))\n",
        "# print(\"unique steps in predicted data:\", len(Counter(chain(*predicted_sequences))))\n",
        "# print((Counter(chain(*decoder_test_target))))\n",
        "# print(Counter(chain(*predicted_sequences)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yed6W_Z0ne10"
      },
      "source": [
        "Lets try out a model with embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbSoF2LAclFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e22882-b863-4063-8cfd-658e0d6aa808"
      },
      "source": [
        "K.clear_session()\n",
        "n_units = [32, 32, 32]\n",
        "emb_dim = 128\n",
        "# credit: https://stackoverflow.com/a/56693548/7737870\n",
        "# latent_dims is an array which defines the depth of the encoder/decoder, as well as how large\n",
        "# the layers should be. So an array of sizes [a,b,c]  would produce a depth-3 encoder and decoder\n",
        "# with layer sizes equal to [c,b,a] and [a,b,c] respectively.\n",
        "\n",
        "# define encoder\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, ), name=\"encoder_input\")\n",
        "embedded_encoder_inputs = layers.Embedding(input_dim=num_encoder_tokens+1, output_dim=emb_dim, input_length=max_encoder_seq_length, name=\"encoder_embedding\")(encoder_inputs)\n",
        "encoder_outputs = embedded_encoder_inputs\n",
        "encoder_states = []\n",
        "for j in range(len(n_units))[::-1]:\n",
        "    encoder_outputs, h, c = layers.LSTM(n_units[j], return_state=True, return_sequences=bool(j), name=f\"encoder_lstm_{len(n_units) - j}\")(encoder_outputs)\n",
        "    encoder_states += [h, c]\n",
        "\n",
        "# define decoder\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, ), name=\"decoder_input\")\n",
        "embedded_decoder_inputs = layers.Embedding(input_dim=num_decoder_tokens+1, output_dim=emb_dim, \\\n",
        "                                           input_length=max_decoder_seq_length, mask_zero=True, name=\"decoder_embedding\")(decoder_inputs)\n",
        "decoder_outputs = embedded_decoder_inputs\n",
        "output_layers = []\n",
        "for j in range(len(n_units)):\n",
        "    output_layers.append( \\\n",
        "        layers.LSTM(n_units[len(n_units) - j - 1], return_sequences=True, return_state=True, name=f\"decoder_lstm_{j+1}\") \\\n",
        "    )\n",
        "    decoder_outputs, dh, dc = output_layers[-1](decoder_outputs, initial_state=encoder_states[2*j:2*(j+1)])\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens+1, activation='softmax', name=\"decoder_dense\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# define training model\n",
        "stacked_embed_model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"stacked_model\")\n",
        "stacked_embed_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"stacked_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      [(None, 29)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_embedding (Embedding)   (None, 4, 128)       6528        encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_embedding (Embedding)   (None, 29, 128)      6784        decoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_1 (LSTM)           [(None, 4, 32), (Non 20608       encoder_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm_1 (LSTM)           [(None, 29, 32), (No 20608       decoder_embedding[0][0]          \n",
            "                                                                 encoder_lstm_1[0][1]             \n",
            "                                                                 encoder_lstm_1[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_2 (LSTM)           [(None, 4, 32), (Non 8320        encoder_lstm_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm_2 (LSTM)           [(None, 29, 32), (No 8320        decoder_lstm_1[0][0]             \n",
            "                                                                 encoder_lstm_2[0][1]             \n",
            "                                                                 encoder_lstm_2[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm_3 (LSTM)           [(None, 32), (None,  8320        encoder_lstm_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm_3 (LSTM)           [(None, 29, 32), (No 8320        decoder_lstm_2[0][0]             \n",
            "                                                                 encoder_lstm_3[0][1]             \n",
            "                                                                 encoder_lstm_3[0][2]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_dense (Dense)           (None, 29, 53)       1749        decoder_lstm_3[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 89,557\n",
            "Trainable params: 89,557\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1hJQFmwpmYo"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "adam = Adam(learning_rate=0.1)\n",
        "stacked_embed_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2-t_NuxpsJn",
        "outputId": "f41240e1-4440-42e2-f62d-0780c749158c"
      },
      "source": [
        "K.clear_session()\n",
        "# fit\n",
        "encoder_train_input_np = np.array(encoder_train_input)\n",
        "decoder_train_input_np = np.array(decoder_train_input)\n",
        "history = stacked_embed_model.fit( \\\n",
        "            [encoder_train_input_np, decoder_train_input_np], decoder_train_target_oh, \\\n",
        "            batch_size=64, \\\n",
        "            epochs=500, \\\n",
        "            validation_split=0.1\n",
        "        )\n",
        "stacked_embed_model.save(\"s2s_stacked_embed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "11/11 [==============================] - 23s 744ms/step - loss: 1.3638 - accuracy: 0.0468 - val_loss: 1.4670 - val_accuracy: 0.0541\n",
            "Epoch 2/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.3946 - accuracy: 0.0631 - val_loss: 1.3930 - val_accuracy: 0.0590\n",
            "Epoch 3/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.2931 - accuracy: 0.0677 - val_loss: 1.2982 - val_accuracy: 0.1032\n",
            "Epoch 4/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.2502 - accuracy: 0.1099 - val_loss: 1.2585 - val_accuracy: 0.1057\n",
            "Epoch 5/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1720 - accuracy: 0.1139 - val_loss: 1.2463 - val_accuracy: 0.1106\n",
            "Epoch 6/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1590 - accuracy: 0.1122 - val_loss: 1.2429 - val_accuracy: 0.1118\n",
            "Epoch 7/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1620 - accuracy: 0.1091 - val_loss: 1.2402 - val_accuracy: 0.1130\n",
            "Epoch 8/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.1606 - accuracy: 0.1109 - val_loss: 1.2389 - val_accuracy: 0.1143\n",
            "Epoch 9/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1853 - accuracy: 0.1133 - val_loss: 1.2380 - val_accuracy: 0.1179\n",
            "Epoch 10/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1642 - accuracy: 0.1131 - val_loss: 1.2364 - val_accuracy: 0.1130\n",
            "Epoch 11/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1862 - accuracy: 0.1098 - val_loss: 1.2353 - val_accuracy: 0.1167\n",
            "Epoch 12/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.1976 - accuracy: 0.1107 - val_loss: 1.2349 - val_accuracy: 0.1179\n",
            "Epoch 13/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1683 - accuracy: 0.1101 - val_loss: 1.2339 - val_accuracy: 0.1216\n",
            "Epoch 14/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1673 - accuracy: 0.1119 - val_loss: 1.2335 - val_accuracy: 0.1216\n",
            "Epoch 15/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1637 - accuracy: 0.1154 - val_loss: 1.2336 - val_accuracy: 0.1167\n",
            "Epoch 16/500\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 1.1738 - accuracy: 0.1101 - val_loss: 1.2317 - val_accuracy: 0.1155\n",
            "Epoch 17/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1447 - accuracy: 0.1114 - val_loss: 1.2332 - val_accuracy: 0.1069\n",
            "Epoch 18/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1873 - accuracy: 0.1138 - val_loss: 1.2311 - val_accuracy: 0.1081\n",
            "Epoch 19/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1688 - accuracy: 0.1166 - val_loss: 1.2316 - val_accuracy: 0.1106\n",
            "Epoch 20/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1682 - accuracy: 0.1182 - val_loss: 1.2307 - val_accuracy: 0.1130\n",
            "Epoch 21/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1704 - accuracy: 0.1178 - val_loss: 1.2281 - val_accuracy: 0.1106\n",
            "Epoch 22/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1594 - accuracy: 0.1184 - val_loss: 1.2278 - val_accuracy: 0.1167\n",
            "Epoch 23/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1636 - accuracy: 0.1162 - val_loss: 1.2233 - val_accuracy: 0.1179\n",
            "Epoch 24/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1255 - accuracy: 0.1190 - val_loss: 1.2192 - val_accuracy: 0.1179\n",
            "Epoch 25/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.1453 - accuracy: 0.1235 - val_loss: 1.2146 - val_accuracy: 0.1204\n",
            "Epoch 26/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.1420 - accuracy: 0.1216 - val_loss: 1.2092 - val_accuracy: 0.1216\n",
            "Epoch 27/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1687 - accuracy: 0.1231 - val_loss: 1.2033 - val_accuracy: 0.1167\n",
            "Epoch 28/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.1149 - accuracy: 0.1205 - val_loss: 1.1952 - val_accuracy: 0.1192\n",
            "Epoch 29/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1374 - accuracy: 0.1257 - val_loss: 1.1881 - val_accuracy: 0.1216\n",
            "Epoch 30/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0885 - accuracy: 0.1286 - val_loss: 1.1893 - val_accuracy: 0.1327\n",
            "Epoch 31/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.1421 - accuracy: 0.1301 - val_loss: 1.1749 - val_accuracy: 0.1339\n",
            "Epoch 32/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0800 - accuracy: 0.1389 - val_loss: 1.1733 - val_accuracy: 0.1302\n",
            "Epoch 33/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.1218 - accuracy: 0.1397 - val_loss: 1.1673 - val_accuracy: 0.1388\n",
            "Epoch 34/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.0665 - accuracy: 0.1447 - val_loss: 1.1626 - val_accuracy: 0.1376\n",
            "Epoch 35/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0783 - accuracy: 0.1425 - val_loss: 1.1588 - val_accuracy: 0.1388\n",
            "Epoch 36/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0581 - accuracy: 0.1558 - val_loss: 1.1535 - val_accuracy: 0.1413\n",
            "Epoch 37/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0940 - accuracy: 0.1532 - val_loss: 1.1501 - val_accuracy: 0.1486\n",
            "Epoch 38/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0878 - accuracy: 0.1537 - val_loss: 1.1457 - val_accuracy: 0.1511\n",
            "Epoch 39/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.0814 - accuracy: 0.1554 - val_loss: 1.1423 - val_accuracy: 0.1536\n",
            "Epoch 40/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0942 - accuracy: 0.1617 - val_loss: 1.1354 - val_accuracy: 0.1548\n",
            "Epoch 41/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0370 - accuracy: 0.1669 - val_loss: 1.1364 - val_accuracy: 0.1585\n",
            "Epoch 42/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0052 - accuracy: 0.1601 - val_loss: 1.1272 - val_accuracy: 0.1597\n",
            "Epoch 43/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0644 - accuracy: 0.1667 - val_loss: 1.1241 - val_accuracy: 0.1499\n",
            "Epoch 44/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0342 - accuracy: 0.1658 - val_loss: 1.1212 - val_accuracy: 0.1499\n",
            "Epoch 45/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.9882 - accuracy: 0.1758 - val_loss: 1.1152 - val_accuracy: 0.1572\n",
            "Epoch 46/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.0165 - accuracy: 0.1701 - val_loss: 1.1135 - val_accuracy: 0.1658\n",
            "Epoch 47/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0201 - accuracy: 0.1793 - val_loss: 1.1115 - val_accuracy: 0.1622\n",
            "Epoch 48/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0288 - accuracy: 0.1752 - val_loss: 1.1092 - val_accuracy: 0.1523\n",
            "Epoch 49/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0310 - accuracy: 0.1758 - val_loss: 1.1053 - val_accuracy: 0.1683\n",
            "Epoch 50/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0540 - accuracy: 0.1786 - val_loss: 1.1028 - val_accuracy: 0.1708\n",
            "Epoch 51/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.0208 - accuracy: 0.1916 - val_loss: 1.1008 - val_accuracy: 0.1744\n",
            "Epoch 52/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.0145 - accuracy: 0.1896 - val_loss: 1.0983 - val_accuracy: 0.1769\n",
            "Epoch 53/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0024 - accuracy: 0.1884 - val_loss: 1.0946 - val_accuracy: 0.1720\n",
            "Epoch 54/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0137 - accuracy: 0.1934 - val_loss: 1.0969 - val_accuracy: 0.1830\n",
            "Epoch 55/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0087 - accuracy: 0.1932 - val_loss: 1.0870 - val_accuracy: 0.1806\n",
            "Epoch 56/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.9965 - accuracy: 0.1914 - val_loss: 1.0856 - val_accuracy: 0.1830\n",
            "Epoch 57/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9933 - accuracy: 0.2000 - val_loss: 1.0811 - val_accuracy: 0.1916\n",
            "Epoch 58/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.9696 - accuracy: 0.2100 - val_loss: 1.0798 - val_accuracy: 0.1781\n",
            "Epoch 59/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9841 - accuracy: 0.2011 - val_loss: 1.0784 - val_accuracy: 0.1843\n",
            "Epoch 60/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.9812 - accuracy: 0.2144 - val_loss: 1.0778 - val_accuracy: 0.1843\n",
            "Epoch 61/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9571 - accuracy: 0.2094 - val_loss: 1.0733 - val_accuracy: 0.1880\n",
            "Epoch 62/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9513 - accuracy: 0.2068 - val_loss: 1.0746 - val_accuracy: 0.1904\n",
            "Epoch 63/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9713 - accuracy: 0.2098 - val_loss: 1.0705 - val_accuracy: 0.1990\n",
            "Epoch 64/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.9445 - accuracy: 0.2092 - val_loss: 1.0657 - val_accuracy: 0.1941\n",
            "Epoch 65/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.9409 - accuracy: 0.2211 - val_loss: 1.0649 - val_accuracy: 0.1953\n",
            "Epoch 66/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9685 - accuracy: 0.2154 - val_loss: 1.0639 - val_accuracy: 0.2002\n",
            "Epoch 67/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.9494 - accuracy: 0.2197 - val_loss: 1.0618 - val_accuracy: 0.2002\n",
            "Epoch 68/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9468 - accuracy: 0.2227 - val_loss: 1.0599 - val_accuracy: 0.1904\n",
            "Epoch 69/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9896 - accuracy: 0.2198 - val_loss: 1.0569 - val_accuracy: 0.2027\n",
            "Epoch 70/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9617 - accuracy: 0.2260 - val_loss: 1.0570 - val_accuracy: 0.1966\n",
            "Epoch 71/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.9364 - accuracy: 0.2294 - val_loss: 1.0544 - val_accuracy: 0.1953\n",
            "Epoch 72/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9275 - accuracy: 0.2282 - val_loss: 1.0566 - val_accuracy: 0.2015\n",
            "Epoch 73/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.9620 - accuracy: 0.2265 - val_loss: 1.0518 - val_accuracy: 0.1855\n",
            "Epoch 74/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.9519 - accuracy: 0.2280 - val_loss: 1.0470 - val_accuracy: 0.2002\n",
            "Epoch 75/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9016 - accuracy: 0.2305 - val_loss: 1.0497 - val_accuracy: 0.1916\n",
            "Epoch 76/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9301 - accuracy: 0.2324 - val_loss: 1.0444 - val_accuracy: 0.1990\n",
            "Epoch 77/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.9092 - accuracy: 0.2390 - val_loss: 1.0458 - val_accuracy: 0.1966\n",
            "Epoch 78/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.9395 - accuracy: 0.2390 - val_loss: 1.0447 - val_accuracy: 0.2015\n",
            "Epoch 79/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.9080 - accuracy: 0.2430 - val_loss: 1.0406 - val_accuracy: 0.2113\n",
            "Epoch 80/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.9240 - accuracy: 0.2405 - val_loss: 1.0390 - val_accuracy: 0.2015\n",
            "Epoch 81/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8956 - accuracy: 0.2415 - val_loss: 1.0378 - val_accuracy: 0.2064\n",
            "Epoch 82/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9195 - accuracy: 0.2412 - val_loss: 1.0380 - val_accuracy: 0.2064\n",
            "Epoch 83/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.9120 - accuracy: 0.2492 - val_loss: 1.0366 - val_accuracy: 0.2052\n",
            "Epoch 84/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.9206 - accuracy: 0.2444 - val_loss: 1.0349 - val_accuracy: 0.2150\n",
            "Epoch 85/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.9100 - accuracy: 0.2440 - val_loss: 1.0374 - val_accuracy: 0.2064\n",
            "Epoch 86/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8719 - accuracy: 0.2535 - val_loss: 1.0338 - val_accuracy: 0.2027\n",
            "Epoch 87/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.8831 - accuracy: 0.2519 - val_loss: 1.0323 - val_accuracy: 0.2113\n",
            "Epoch 88/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8627 - accuracy: 0.2578 - val_loss: 1.0377 - val_accuracy: 0.2039\n",
            "Epoch 89/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8874 - accuracy: 0.2636 - val_loss: 1.0357 - val_accuracy: 0.2113\n",
            "Epoch 90/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8699 - accuracy: 0.2531 - val_loss: 1.0321 - val_accuracy: 0.2064\n",
            "Epoch 91/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8677 - accuracy: 0.2659 - val_loss: 1.0316 - val_accuracy: 0.2150\n",
            "Epoch 92/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8818 - accuracy: 0.2622 - val_loss: 1.0313 - val_accuracy: 0.2150\n",
            "Epoch 93/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8523 - accuracy: 0.2664 - val_loss: 1.0372 - val_accuracy: 0.2138\n",
            "Epoch 94/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8638 - accuracy: 0.2638 - val_loss: 1.0265 - val_accuracy: 0.2064\n",
            "Epoch 95/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8455 - accuracy: 0.2717 - val_loss: 1.0303 - val_accuracy: 0.2113\n",
            "Epoch 96/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8804 - accuracy: 0.2699 - val_loss: 1.0310 - val_accuracy: 0.2150\n",
            "Epoch 97/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8569 - accuracy: 0.2648 - val_loss: 1.0266 - val_accuracy: 0.2088\n",
            "Epoch 98/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.8527 - accuracy: 0.2764 - val_loss: 1.0315 - val_accuracy: 0.2150\n",
            "Epoch 99/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8390 - accuracy: 0.2813 - val_loss: 1.0302 - val_accuracy: 0.2101\n",
            "Epoch 100/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8594 - accuracy: 0.2681 - val_loss: 1.0291 - val_accuracy: 0.2088\n",
            "Epoch 101/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8672 - accuracy: 0.2754 - val_loss: 1.0299 - val_accuracy: 0.2150\n",
            "Epoch 102/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8499 - accuracy: 0.2837 - val_loss: 1.0347 - val_accuracy: 0.2113\n",
            "Epoch 103/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8361 - accuracy: 0.2872 - val_loss: 1.0320 - val_accuracy: 0.2113\n",
            "Epoch 104/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8606 - accuracy: 0.2787 - val_loss: 1.0269 - val_accuracy: 0.2138\n",
            "Epoch 105/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8184 - accuracy: 0.2898 - val_loss: 1.0324 - val_accuracy: 0.2076\n",
            "Epoch 106/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8437 - accuracy: 0.2835 - val_loss: 1.0313 - val_accuracy: 0.2125\n",
            "Epoch 107/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8478 - accuracy: 0.2804 - val_loss: 1.0281 - val_accuracy: 0.2125\n",
            "Epoch 108/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.8451 - accuracy: 0.2807 - val_loss: 1.0323 - val_accuracy: 0.2052\n",
            "Epoch 109/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8506 - accuracy: 0.2826 - val_loss: 1.0303 - val_accuracy: 0.2187\n",
            "Epoch 110/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8574 - accuracy: 0.2926 - val_loss: 1.0313 - val_accuracy: 0.2076\n",
            "Epoch 111/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8441 - accuracy: 0.2949 - val_loss: 1.0288 - val_accuracy: 0.2101\n",
            "Epoch 112/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8098 - accuracy: 0.2930 - val_loss: 1.0311 - val_accuracy: 0.2224\n",
            "Epoch 113/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8460 - accuracy: 0.2889 - val_loss: 1.0321 - val_accuracy: 0.2162\n",
            "Epoch 114/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8174 - accuracy: 0.2901 - val_loss: 1.0368 - val_accuracy: 0.2076\n",
            "Epoch 115/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8297 - accuracy: 0.3023 - val_loss: 1.0335 - val_accuracy: 0.2199\n",
            "Epoch 116/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8327 - accuracy: 0.2958 - val_loss: 1.0346 - val_accuracy: 0.2113\n",
            "Epoch 117/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8298 - accuracy: 0.2998 - val_loss: 1.0404 - val_accuracy: 0.2088\n",
            "Epoch 118/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8318 - accuracy: 0.3031 - val_loss: 1.0307 - val_accuracy: 0.2150\n",
            "Epoch 119/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.8231 - accuracy: 0.3061 - val_loss: 1.0361 - val_accuracy: 0.2150\n",
            "Epoch 120/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.8063 - accuracy: 0.3127 - val_loss: 1.0458 - val_accuracy: 0.2125\n",
            "Epoch 121/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.8086 - accuracy: 0.3083 - val_loss: 1.0392 - val_accuracy: 0.2211\n",
            "Epoch 122/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8414 - accuracy: 0.3090 - val_loss: 1.0323 - val_accuracy: 0.2162\n",
            "Epoch 123/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.8335 - accuracy: 0.3086 - val_loss: 1.0310 - val_accuracy: 0.2174\n",
            "Epoch 124/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7935 - accuracy: 0.3168 - val_loss: 1.0485 - val_accuracy: 0.2088\n",
            "Epoch 125/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7713 - accuracy: 0.3206 - val_loss: 1.0384 - val_accuracy: 0.2125\n",
            "Epoch 126/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7879 - accuracy: 0.3179 - val_loss: 1.0308 - val_accuracy: 0.2224\n",
            "Epoch 127/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.8211 - accuracy: 0.3125 - val_loss: 1.0555 - val_accuracy: 0.2125\n",
            "Epoch 128/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8214 - accuracy: 0.3195 - val_loss: 1.0389 - val_accuracy: 0.2248\n",
            "Epoch 129/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.8006 - accuracy: 0.3185 - val_loss: 1.0513 - val_accuracy: 0.2088\n",
            "Epoch 130/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7785 - accuracy: 0.3259 - val_loss: 1.0436 - val_accuracy: 0.2224\n",
            "Epoch 131/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.7765 - accuracy: 0.3244 - val_loss: 1.0533 - val_accuracy: 0.2174\n",
            "Epoch 132/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7882 - accuracy: 0.3294 - val_loss: 1.0586 - val_accuracy: 0.2125\n",
            "Epoch 133/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7598 - accuracy: 0.3372 - val_loss: 1.0508 - val_accuracy: 0.2187\n",
            "Epoch 134/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7874 - accuracy: 0.3331 - val_loss: 1.0602 - val_accuracy: 0.2088\n",
            "Epoch 135/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7731 - accuracy: 0.3427 - val_loss: 1.0606 - val_accuracy: 0.2224\n",
            "Epoch 136/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.7949 - accuracy: 0.3253 - val_loss: 1.0700 - val_accuracy: 0.2273\n",
            "Epoch 137/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7574 - accuracy: 0.3418 - val_loss: 1.0593 - val_accuracy: 0.2211\n",
            "Epoch 138/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7560 - accuracy: 0.3457 - val_loss: 1.0698 - val_accuracy: 0.2162\n",
            "Epoch 139/500\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.7700 - accuracy: 0.3402 - val_loss: 1.0653 - val_accuracy: 0.2174\n",
            "Epoch 140/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7876 - accuracy: 0.3435 - val_loss: 1.0737 - val_accuracy: 0.2125\n",
            "Epoch 141/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7356 - accuracy: 0.3499 - val_loss: 1.0586 - val_accuracy: 0.2260\n",
            "Epoch 142/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7509 - accuracy: 0.3525 - val_loss: 1.0668 - val_accuracy: 0.2248\n",
            "Epoch 143/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7642 - accuracy: 0.3484 - val_loss: 1.0732 - val_accuracy: 0.2199\n",
            "Epoch 144/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7393 - accuracy: 0.3603 - val_loss: 1.0683 - val_accuracy: 0.2150\n",
            "Epoch 145/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7449 - accuracy: 0.3565 - val_loss: 1.0893 - val_accuracy: 0.2150\n",
            "Epoch 146/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7403 - accuracy: 0.3597 - val_loss: 1.0745 - val_accuracy: 0.2187\n",
            "Epoch 147/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7611 - accuracy: 0.3507 - val_loss: 1.0828 - val_accuracy: 0.2101\n",
            "Epoch 148/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7589 - accuracy: 0.3598 - val_loss: 1.0870 - val_accuracy: 0.2150\n",
            "Epoch 149/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7806 - accuracy: 0.3556 - val_loss: 1.0847 - val_accuracy: 0.2138\n",
            "Epoch 150/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7531 - accuracy: 0.3513 - val_loss: 1.0783 - val_accuracy: 0.2162\n",
            "Epoch 151/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7438 - accuracy: 0.3559 - val_loss: 1.0850 - val_accuracy: 0.2162\n",
            "Epoch 152/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7576 - accuracy: 0.3536 - val_loss: 1.0797 - val_accuracy: 0.2088\n",
            "Epoch 153/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7391 - accuracy: 0.3567 - val_loss: 1.0885 - val_accuracy: 0.2236\n",
            "Epoch 154/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7381 - accuracy: 0.3634 - val_loss: 1.0783 - val_accuracy: 0.2150\n",
            "Epoch 155/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7275 - accuracy: 0.3609 - val_loss: 1.0885 - val_accuracy: 0.2187\n",
            "Epoch 156/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7090 - accuracy: 0.3712 - val_loss: 1.1007 - val_accuracy: 0.2064\n",
            "Epoch 157/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7255 - accuracy: 0.3653 - val_loss: 1.1042 - val_accuracy: 0.2064\n",
            "Epoch 158/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7734 - accuracy: 0.3636 - val_loss: 1.0873 - val_accuracy: 0.2236\n",
            "Epoch 159/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7261 - accuracy: 0.3673 - val_loss: 1.1059 - val_accuracy: 0.2260\n",
            "Epoch 160/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7450 - accuracy: 0.3678 - val_loss: 1.1096 - val_accuracy: 0.2138\n",
            "Epoch 161/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7685 - accuracy: 0.3610 - val_loss: 1.1002 - val_accuracy: 0.2125\n",
            "Epoch 162/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7435 - accuracy: 0.3681 - val_loss: 1.0990 - val_accuracy: 0.2162\n",
            "Epoch 163/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7358 - accuracy: 0.3650 - val_loss: 1.1017 - val_accuracy: 0.2211\n",
            "Epoch 164/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7257 - accuracy: 0.3717 - val_loss: 1.1171 - val_accuracy: 0.2052\n",
            "Epoch 165/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7415 - accuracy: 0.3754 - val_loss: 1.0996 - val_accuracy: 0.2162\n",
            "Epoch 166/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7180 - accuracy: 0.3772 - val_loss: 1.1094 - val_accuracy: 0.2125\n",
            "Epoch 167/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7451 - accuracy: 0.3739 - val_loss: 1.1150 - val_accuracy: 0.2101\n",
            "Epoch 168/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7285 - accuracy: 0.3810 - val_loss: 1.1104 - val_accuracy: 0.2260\n",
            "Epoch 169/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6993 - accuracy: 0.3823 - val_loss: 1.1234 - val_accuracy: 0.2187\n",
            "Epoch 170/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6797 - accuracy: 0.3952 - val_loss: 1.1179 - val_accuracy: 0.2199\n",
            "Epoch 171/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7331 - accuracy: 0.3818 - val_loss: 1.1193 - val_accuracy: 0.2138\n",
            "Epoch 172/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7006 - accuracy: 0.3885 - val_loss: 1.1346 - val_accuracy: 0.2027\n",
            "Epoch 173/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7187 - accuracy: 0.3837 - val_loss: 1.1363 - val_accuracy: 0.2039\n",
            "Epoch 174/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7021 - accuracy: 0.3910 - val_loss: 1.1306 - val_accuracy: 0.2138\n",
            "Epoch 175/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7057 - accuracy: 0.3902 - val_loss: 1.1349 - val_accuracy: 0.2088\n",
            "Epoch 176/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7111 - accuracy: 0.3875 - val_loss: 1.1352 - val_accuracy: 0.1941\n",
            "Epoch 177/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.7112 - accuracy: 0.3884 - val_loss: 1.1381 - val_accuracy: 0.2088\n",
            "Epoch 178/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.6759 - accuracy: 0.4026 - val_loss: 1.1494 - val_accuracy: 0.2015\n",
            "Epoch 179/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.7122 - accuracy: 0.3994 - val_loss: 1.1354 - val_accuracy: 0.2101\n",
            "Epoch 180/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.7097 - accuracy: 0.3869 - val_loss: 1.1558 - val_accuracy: 0.2039\n",
            "Epoch 181/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6941 - accuracy: 0.3993 - val_loss: 1.1502 - val_accuracy: 0.2052\n",
            "Epoch 182/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6786 - accuracy: 0.4072 - val_loss: 1.1561 - val_accuracy: 0.2052\n",
            "Epoch 183/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7057 - accuracy: 0.3874 - val_loss: 1.1601 - val_accuracy: 0.2113\n",
            "Epoch 184/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6737 - accuracy: 0.4089 - val_loss: 1.1625 - val_accuracy: 0.1990\n",
            "Epoch 185/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6803 - accuracy: 0.3941 - val_loss: 1.1594 - val_accuracy: 0.1990\n",
            "Epoch 186/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6993 - accuracy: 0.3985 - val_loss: 1.1769 - val_accuracy: 0.2064\n",
            "Epoch 187/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6834 - accuracy: 0.3948 - val_loss: 1.1593 - val_accuracy: 0.2064\n",
            "Epoch 188/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.7267 - accuracy: 0.3960 - val_loss: 1.1557 - val_accuracy: 0.2138\n",
            "Epoch 189/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6795 - accuracy: 0.4105 - val_loss: 1.1752 - val_accuracy: 0.1990\n",
            "Epoch 190/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6755 - accuracy: 0.4176 - val_loss: 1.1717 - val_accuracy: 0.2015\n",
            "Epoch 191/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6960 - accuracy: 0.4125 - val_loss: 1.1850 - val_accuracy: 0.1990\n",
            "Epoch 192/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6663 - accuracy: 0.4156 - val_loss: 1.1857 - val_accuracy: 0.2101\n",
            "Epoch 193/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6713 - accuracy: 0.4058 - val_loss: 1.1780 - val_accuracy: 0.2039\n",
            "Epoch 194/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6628 - accuracy: 0.4163 - val_loss: 1.1858 - val_accuracy: 0.1953\n",
            "Epoch 195/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6689 - accuracy: 0.4107 - val_loss: 1.1987 - val_accuracy: 0.1916\n",
            "Epoch 196/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6833 - accuracy: 0.4108 - val_loss: 1.2007 - val_accuracy: 0.2015\n",
            "Epoch 197/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.6489 - accuracy: 0.4210 - val_loss: 1.2163 - val_accuracy: 0.2015\n",
            "Epoch 198/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6912 - accuracy: 0.4104 - val_loss: 1.2007 - val_accuracy: 0.2052\n",
            "Epoch 199/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6527 - accuracy: 0.4170 - val_loss: 1.2048 - val_accuracy: 0.1978\n",
            "Epoch 200/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6498 - accuracy: 0.4184 - val_loss: 1.1921 - val_accuracy: 0.2015\n",
            "Epoch 201/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6573 - accuracy: 0.4231 - val_loss: 1.2291 - val_accuracy: 0.1978\n",
            "Epoch 202/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6718 - accuracy: 0.4253 - val_loss: 1.2178 - val_accuracy: 0.2002\n",
            "Epoch 203/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6652 - accuracy: 0.4335 - val_loss: 1.2305 - val_accuracy: 0.1941\n",
            "Epoch 204/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6855 - accuracy: 0.4206 - val_loss: 1.2342 - val_accuracy: 0.2039\n",
            "Epoch 205/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6429 - accuracy: 0.4307 - val_loss: 1.2304 - val_accuracy: 0.2015\n",
            "Epoch 206/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6450 - accuracy: 0.4308 - val_loss: 1.2207 - val_accuracy: 0.1953\n",
            "Epoch 207/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6848 - accuracy: 0.4268 - val_loss: 1.2450 - val_accuracy: 0.1904\n",
            "Epoch 208/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6430 - accuracy: 0.4273 - val_loss: 1.2347 - val_accuracy: 0.2002\n",
            "Epoch 209/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6495 - accuracy: 0.4283 - val_loss: 1.2277 - val_accuracy: 0.2039\n",
            "Epoch 210/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6488 - accuracy: 0.4230 - val_loss: 1.2376 - val_accuracy: 0.1953\n",
            "Epoch 211/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6574 - accuracy: 0.4233 - val_loss: 1.2549 - val_accuracy: 0.1990\n",
            "Epoch 212/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6447 - accuracy: 0.4408 - val_loss: 1.2394 - val_accuracy: 0.1941\n",
            "Epoch 213/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6315 - accuracy: 0.4430 - val_loss: 1.2431 - val_accuracy: 0.2015\n",
            "Epoch 214/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6413 - accuracy: 0.4453 - val_loss: 1.2508 - val_accuracy: 0.1953\n",
            "Epoch 215/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6360 - accuracy: 0.4331 - val_loss: 1.2606 - val_accuracy: 0.1978\n",
            "Epoch 216/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6550 - accuracy: 0.4321 - val_loss: 1.2496 - val_accuracy: 0.1953\n",
            "Epoch 217/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6449 - accuracy: 0.4372 - val_loss: 1.2554 - val_accuracy: 0.1990\n",
            "Epoch 218/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6375 - accuracy: 0.4399 - val_loss: 1.2589 - val_accuracy: 0.1966\n",
            "Epoch 219/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6374 - accuracy: 0.4413 - val_loss: 1.2588 - val_accuracy: 0.1966\n",
            "Epoch 220/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6402 - accuracy: 0.4380 - val_loss: 1.2557 - val_accuracy: 0.1990\n",
            "Epoch 221/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6221 - accuracy: 0.4514 - val_loss: 1.2525 - val_accuracy: 0.1941\n",
            "Epoch 222/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6094 - accuracy: 0.4527 - val_loss: 1.2664 - val_accuracy: 0.1953\n",
            "Epoch 223/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6206 - accuracy: 0.4556 - val_loss: 1.2760 - val_accuracy: 0.1929\n",
            "Epoch 224/500\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6233 - accuracy: 0.4476 - val_loss: 1.2793 - val_accuracy: 0.1953\n",
            "Epoch 225/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6299 - accuracy: 0.4557 - val_loss: 1.2811 - val_accuracy: 0.1892\n",
            "Epoch 226/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6160 - accuracy: 0.4557 - val_loss: 1.2828 - val_accuracy: 0.1904\n",
            "Epoch 227/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6181 - accuracy: 0.4458 - val_loss: 1.2642 - val_accuracy: 0.1880\n",
            "Epoch 228/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6196 - accuracy: 0.4464 - val_loss: 1.2766 - val_accuracy: 0.2002\n",
            "Epoch 229/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6043 - accuracy: 0.4564 - val_loss: 1.2958 - val_accuracy: 0.1916\n",
            "Epoch 230/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6113 - accuracy: 0.4554 - val_loss: 1.2678 - val_accuracy: 0.1978\n",
            "Epoch 231/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6370 - accuracy: 0.4510 - val_loss: 1.2927 - val_accuracy: 0.1880\n",
            "Epoch 232/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6199 - accuracy: 0.4623 - val_loss: 1.2824 - val_accuracy: 0.1904\n",
            "Epoch 233/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6179 - accuracy: 0.4622 - val_loss: 1.3021 - val_accuracy: 0.1941\n",
            "Epoch 234/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.6210 - accuracy: 0.4585 - val_loss: 1.2914 - val_accuracy: 0.2002\n",
            "Epoch 235/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6022 - accuracy: 0.4611 - val_loss: 1.2969 - val_accuracy: 0.1966\n",
            "Epoch 236/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5950 - accuracy: 0.4627 - val_loss: 1.3161 - val_accuracy: 0.1892\n",
            "Epoch 237/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6133 - accuracy: 0.4671 - val_loss: 1.2913 - val_accuracy: 0.1904\n",
            "Epoch 238/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6062 - accuracy: 0.4563 - val_loss: 1.2989 - val_accuracy: 0.1892\n",
            "Epoch 239/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6203 - accuracy: 0.4591 - val_loss: 1.2874 - val_accuracy: 0.2015\n",
            "Epoch 240/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5902 - accuracy: 0.4700 - val_loss: 1.3036 - val_accuracy: 0.1904\n",
            "Epoch 241/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.6042 - accuracy: 0.4792 - val_loss: 1.3142 - val_accuracy: 0.1916\n",
            "Epoch 242/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6041 - accuracy: 0.4696 - val_loss: 1.3063 - val_accuracy: 0.1892\n",
            "Epoch 243/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6296 - accuracy: 0.4606 - val_loss: 1.3107 - val_accuracy: 0.1867\n",
            "Epoch 244/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6076 - accuracy: 0.4636 - val_loss: 1.3209 - val_accuracy: 0.1892\n",
            "Epoch 245/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5978 - accuracy: 0.4671 - val_loss: 1.3182 - val_accuracy: 0.1916\n",
            "Epoch 246/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5933 - accuracy: 0.4740 - val_loss: 1.3174 - val_accuracy: 0.1867\n",
            "Epoch 247/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6057 - accuracy: 0.4746 - val_loss: 1.3232 - val_accuracy: 0.1916\n",
            "Epoch 248/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6029 - accuracy: 0.4732 - val_loss: 1.3197 - val_accuracy: 0.1855\n",
            "Epoch 249/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5977 - accuracy: 0.4808 - val_loss: 1.3309 - val_accuracy: 0.1880\n",
            "Epoch 250/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5906 - accuracy: 0.4722 - val_loss: 1.3187 - val_accuracy: 0.1830\n",
            "Epoch 251/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6159 - accuracy: 0.4713 - val_loss: 1.3398 - val_accuracy: 0.1892\n",
            "Epoch 252/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6094 - accuracy: 0.4770 - val_loss: 1.3427 - val_accuracy: 0.1867\n",
            "Epoch 253/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5861 - accuracy: 0.4903 - val_loss: 1.3512 - val_accuracy: 0.1880\n",
            "Epoch 254/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5928 - accuracy: 0.4854 - val_loss: 1.3414 - val_accuracy: 0.1769\n",
            "Epoch 255/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5931 - accuracy: 0.4843 - val_loss: 1.3515 - val_accuracy: 0.1855\n",
            "Epoch 256/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5864 - accuracy: 0.4884 - val_loss: 1.3419 - val_accuracy: 0.1892\n",
            "Epoch 257/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5975 - accuracy: 0.4777 - val_loss: 1.3435 - val_accuracy: 0.1855\n",
            "Epoch 258/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5810 - accuracy: 0.4858 - val_loss: 1.3697 - val_accuracy: 0.1929\n",
            "Epoch 259/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5936 - accuracy: 0.4826 - val_loss: 1.3687 - val_accuracy: 0.1806\n",
            "Epoch 260/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5769 - accuracy: 0.4980 - val_loss: 1.3715 - val_accuracy: 0.1830\n",
            "Epoch 261/500\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.5690 - accuracy: 0.4983 - val_loss: 1.3697 - val_accuracy: 0.1904\n",
            "Epoch 262/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.5835 - accuracy: 0.4961 - val_loss: 1.3691 - val_accuracy: 0.1855\n",
            "Epoch 263/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5606 - accuracy: 0.4976 - val_loss: 1.3860 - val_accuracy: 0.1843\n",
            "Epoch 264/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5825 - accuracy: 0.4885 - val_loss: 1.3712 - val_accuracy: 0.1929\n",
            "Epoch 265/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5667 - accuracy: 0.4956 - val_loss: 1.3715 - val_accuracy: 0.1794\n",
            "Epoch 266/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5823 - accuracy: 0.4921 - val_loss: 1.3923 - val_accuracy: 0.1855\n",
            "Epoch 267/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5514 - accuracy: 0.5077 - val_loss: 1.3866 - val_accuracy: 0.1830\n",
            "Epoch 268/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5783 - accuracy: 0.4966 - val_loss: 1.3937 - val_accuracy: 0.1929\n",
            "Epoch 269/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5728 - accuracy: 0.4975 - val_loss: 1.3706 - val_accuracy: 0.1867\n",
            "Epoch 270/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5616 - accuracy: 0.5058 - val_loss: 1.3898 - val_accuracy: 0.1892\n",
            "Epoch 271/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5993 - accuracy: 0.4953 - val_loss: 1.3894 - val_accuracy: 0.1855\n",
            "Epoch 272/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5848 - accuracy: 0.4968 - val_loss: 1.3901 - val_accuracy: 0.1867\n",
            "Epoch 273/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5575 - accuracy: 0.5051 - val_loss: 1.4079 - val_accuracy: 0.1867\n",
            "Epoch 274/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5619 - accuracy: 0.5019 - val_loss: 1.4052 - val_accuracy: 0.1867\n",
            "Epoch 275/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5717 - accuracy: 0.4999 - val_loss: 1.3940 - val_accuracy: 0.1855\n",
            "Epoch 276/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5443 - accuracy: 0.5016 - val_loss: 1.4078 - val_accuracy: 0.1757\n",
            "Epoch 277/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5373 - accuracy: 0.5167 - val_loss: 1.3958 - val_accuracy: 0.1843\n",
            "Epoch 278/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5342 - accuracy: 0.5100 - val_loss: 1.3868 - val_accuracy: 0.1769\n",
            "Epoch 279/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5661 - accuracy: 0.5060 - val_loss: 1.4151 - val_accuracy: 0.1830\n",
            "Epoch 280/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5646 - accuracy: 0.5093 - val_loss: 1.4085 - val_accuracy: 0.1794\n",
            "Epoch 281/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5644 - accuracy: 0.5089 - val_loss: 1.4262 - val_accuracy: 0.1781\n",
            "Epoch 282/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5818 - accuracy: 0.5058 - val_loss: 1.4122 - val_accuracy: 0.1830\n",
            "Epoch 283/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.5264 - accuracy: 0.5228 - val_loss: 1.4287 - val_accuracy: 0.1806\n",
            "Epoch 284/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5533 - accuracy: 0.5159 - val_loss: 1.4263 - val_accuracy: 0.1830\n",
            "Epoch 285/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5361 - accuracy: 0.5244 - val_loss: 1.4327 - val_accuracy: 0.1806\n",
            "Epoch 286/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5323 - accuracy: 0.5303 - val_loss: 1.4452 - val_accuracy: 0.1757\n",
            "Epoch 287/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5354 - accuracy: 0.5215 - val_loss: 1.4414 - val_accuracy: 0.1732\n",
            "Epoch 288/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5320 - accuracy: 0.5235 - val_loss: 1.4493 - val_accuracy: 0.1720\n",
            "Epoch 289/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5493 - accuracy: 0.5262 - val_loss: 1.4428 - val_accuracy: 0.1769\n",
            "Epoch 290/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5448 - accuracy: 0.5237 - val_loss: 1.4321 - val_accuracy: 0.1843\n",
            "Epoch 291/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5406 - accuracy: 0.5274 - val_loss: 1.4370 - val_accuracy: 0.1769\n",
            "Epoch 292/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5660 - accuracy: 0.5080 - val_loss: 1.4431 - val_accuracy: 0.1720\n",
            "Epoch 293/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5636 - accuracy: 0.5079 - val_loss: 1.4215 - val_accuracy: 0.1683\n",
            "Epoch 294/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5339 - accuracy: 0.5211 - val_loss: 1.4394 - val_accuracy: 0.1634\n",
            "Epoch 295/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5261 - accuracy: 0.5291 - val_loss: 1.4427 - val_accuracy: 0.1695\n",
            "Epoch 296/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5006 - accuracy: 0.5444 - val_loss: 1.4458 - val_accuracy: 0.1720\n",
            "Epoch 297/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5255 - accuracy: 0.5371 - val_loss: 1.4399 - val_accuracy: 0.1732\n",
            "Epoch 298/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5282 - accuracy: 0.5395 - val_loss: 1.4625 - val_accuracy: 0.1744\n",
            "Epoch 299/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5326 - accuracy: 0.5351 - val_loss: 1.4519 - val_accuracy: 0.1708\n",
            "Epoch 300/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5396 - accuracy: 0.5301 - val_loss: 1.4446 - val_accuracy: 0.1757\n",
            "Epoch 301/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5387 - accuracy: 0.5290 - val_loss: 1.4570 - val_accuracy: 0.1609\n",
            "Epoch 302/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5395 - accuracy: 0.5272 - val_loss: 1.4622 - val_accuracy: 0.1658\n",
            "Epoch 303/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.5345 - accuracy: 0.5378 - val_loss: 1.4672 - val_accuracy: 0.1695\n",
            "Epoch 304/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5129 - accuracy: 0.5496 - val_loss: 1.4707 - val_accuracy: 0.1708\n",
            "Epoch 305/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5473 - accuracy: 0.5418 - val_loss: 1.4760 - val_accuracy: 0.1658\n",
            "Epoch 306/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5368 - accuracy: 0.5375 - val_loss: 1.4737 - val_accuracy: 0.1671\n",
            "Epoch 307/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5184 - accuracy: 0.5354 - val_loss: 1.4876 - val_accuracy: 0.1732\n",
            "Epoch 308/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5253 - accuracy: 0.5483 - val_loss: 1.4833 - val_accuracy: 0.1744\n",
            "Epoch 309/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5232 - accuracy: 0.5532 - val_loss: 1.5007 - val_accuracy: 0.1720\n",
            "Epoch 310/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4804 - accuracy: 0.5636 - val_loss: 1.4887 - val_accuracy: 0.1708\n",
            "Epoch 311/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5085 - accuracy: 0.5482 - val_loss: 1.5190 - val_accuracy: 0.1732\n",
            "Epoch 312/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4991 - accuracy: 0.5481 - val_loss: 1.4824 - val_accuracy: 0.1708\n",
            "Epoch 313/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5218 - accuracy: 0.5495 - val_loss: 1.5150 - val_accuracy: 0.1671\n",
            "Epoch 314/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5110 - accuracy: 0.5547 - val_loss: 1.5019 - val_accuracy: 0.1683\n",
            "Epoch 315/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.5287 - accuracy: 0.5518 - val_loss: 1.5104 - val_accuracy: 0.1720\n",
            "Epoch 316/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.5131 - accuracy: 0.5533 - val_loss: 1.5154 - val_accuracy: 0.1732\n",
            "Epoch 317/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4926 - accuracy: 0.5647 - val_loss: 1.5047 - val_accuracy: 0.1683\n",
            "Epoch 318/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4850 - accuracy: 0.5663 - val_loss: 1.5056 - val_accuracy: 0.1720\n",
            "Epoch 319/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.5184 - accuracy: 0.5528 - val_loss: 1.5180 - val_accuracy: 0.1708\n",
            "Epoch 320/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5096 - accuracy: 0.5617 - val_loss: 1.5146 - val_accuracy: 0.1708\n",
            "Epoch 321/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5127 - accuracy: 0.5564 - val_loss: 1.5327 - val_accuracy: 0.1695\n",
            "Epoch 322/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.5080 - accuracy: 0.5535 - val_loss: 1.5265 - val_accuracy: 0.1732\n",
            "Epoch 323/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5011 - accuracy: 0.5670 - val_loss: 1.5085 - val_accuracy: 0.1708\n",
            "Epoch 324/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.5238 - accuracy: 0.5614 - val_loss: 1.5325 - val_accuracy: 0.1695\n",
            "Epoch 325/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.5014 - accuracy: 0.5662 - val_loss: 1.5411 - val_accuracy: 0.1683\n",
            "Epoch 326/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4767 - accuracy: 0.5754 - val_loss: 1.5438 - val_accuracy: 0.1683\n",
            "Epoch 327/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4996 - accuracy: 0.5552 - val_loss: 1.5311 - val_accuracy: 0.1769\n",
            "Epoch 328/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4864 - accuracy: 0.5745 - val_loss: 1.5614 - val_accuracy: 0.1597\n",
            "Epoch 329/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4717 - accuracy: 0.5833 - val_loss: 1.5509 - val_accuracy: 0.1732\n",
            "Epoch 330/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4882 - accuracy: 0.5801 - val_loss: 1.5384 - val_accuracy: 0.1658\n",
            "Epoch 331/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.4830 - accuracy: 0.5782 - val_loss: 1.5404 - val_accuracy: 0.1708\n",
            "Epoch 332/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4904 - accuracy: 0.5792 - val_loss: 1.5588 - val_accuracy: 0.1708\n",
            "Epoch 333/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4956 - accuracy: 0.5733 - val_loss: 1.5802 - val_accuracy: 0.1634\n",
            "Epoch 334/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4996 - accuracy: 0.5666 - val_loss: 1.5447 - val_accuracy: 0.1646\n",
            "Epoch 335/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4874 - accuracy: 0.5704 - val_loss: 1.5364 - val_accuracy: 0.1695\n",
            "Epoch 336/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4880 - accuracy: 0.5662 - val_loss: 1.5469 - val_accuracy: 0.1744\n",
            "Epoch 337/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4751 - accuracy: 0.5854 - val_loss: 1.5488 - val_accuracy: 0.1658\n",
            "Epoch 338/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.4846 - accuracy: 0.5720 - val_loss: 1.5488 - val_accuracy: 0.1720\n",
            "Epoch 339/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4841 - accuracy: 0.5787 - val_loss: 1.5706 - val_accuracy: 0.1732\n",
            "Epoch 340/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4732 - accuracy: 0.5806 - val_loss: 1.5468 - val_accuracy: 0.1634\n",
            "Epoch 341/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4752 - accuracy: 0.5958 - val_loss: 1.5774 - val_accuracy: 0.1757\n",
            "Epoch 342/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.4822 - accuracy: 0.5896 - val_loss: 1.5813 - val_accuracy: 0.1548\n",
            "Epoch 343/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4629 - accuracy: 0.5964 - val_loss: 1.5828 - val_accuracy: 0.1732\n",
            "Epoch 344/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4667 - accuracy: 0.5864 - val_loss: 1.5731 - val_accuracy: 0.1646\n",
            "Epoch 345/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4780 - accuracy: 0.5978 - val_loss: 1.6038 - val_accuracy: 0.1695\n",
            "Epoch 346/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4603 - accuracy: 0.5997 - val_loss: 1.5903 - val_accuracy: 0.1658\n",
            "Epoch 347/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4662 - accuracy: 0.5904 - val_loss: 1.6002 - val_accuracy: 0.1720\n",
            "Epoch 348/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4538 - accuracy: 0.6007 - val_loss: 1.5963 - val_accuracy: 0.1658\n",
            "Epoch 349/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4719 - accuracy: 0.5958 - val_loss: 1.6177 - val_accuracy: 0.1658\n",
            "Epoch 350/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4870 - accuracy: 0.5914 - val_loss: 1.5986 - val_accuracy: 0.1646\n",
            "Epoch 351/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4632 - accuracy: 0.5922 - val_loss: 1.6116 - val_accuracy: 0.1646\n",
            "Epoch 352/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4627 - accuracy: 0.5938 - val_loss: 1.6096 - val_accuracy: 0.1744\n",
            "Epoch 353/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4522 - accuracy: 0.5916 - val_loss: 1.6084 - val_accuracy: 0.1597\n",
            "Epoch 354/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4644 - accuracy: 0.5975 - val_loss: 1.6175 - val_accuracy: 0.1720\n",
            "Epoch 355/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4544 - accuracy: 0.5991 - val_loss: 1.6037 - val_accuracy: 0.1695\n",
            "Epoch 356/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4907 - accuracy: 0.5901 - val_loss: 1.6123 - val_accuracy: 0.1695\n",
            "Epoch 357/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.4576 - accuracy: 0.5882 - val_loss: 1.6272 - val_accuracy: 0.1646\n",
            "Epoch 358/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4648 - accuracy: 0.6007 - val_loss: 1.6174 - val_accuracy: 0.1683\n",
            "Epoch 359/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4582 - accuracy: 0.5992 - val_loss: 1.6109 - val_accuracy: 0.1634\n",
            "Epoch 360/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4527 - accuracy: 0.6022 - val_loss: 1.6360 - val_accuracy: 0.1671\n",
            "Epoch 361/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4656 - accuracy: 0.6075 - val_loss: 1.6261 - val_accuracy: 0.1695\n",
            "Epoch 362/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4585 - accuracy: 0.6004 - val_loss: 1.6546 - val_accuracy: 0.1658\n",
            "Epoch 363/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.4385 - accuracy: 0.6187 - val_loss: 1.6387 - val_accuracy: 0.1646\n",
            "Epoch 364/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4359 - accuracy: 0.6144 - val_loss: 1.6399 - val_accuracy: 0.1695\n",
            "Epoch 365/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4377 - accuracy: 0.6128 - val_loss: 1.6511 - val_accuracy: 0.1671\n",
            "Epoch 366/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4498 - accuracy: 0.6165 - val_loss: 1.6499 - val_accuracy: 0.1720\n",
            "Epoch 367/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4382 - accuracy: 0.6124 - val_loss: 1.6604 - val_accuracy: 0.1634\n",
            "Epoch 368/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4434 - accuracy: 0.6103 - val_loss: 1.6651 - val_accuracy: 0.1683\n",
            "Epoch 369/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4337 - accuracy: 0.6134 - val_loss: 1.6406 - val_accuracy: 0.1658\n",
            "Epoch 370/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4632 - accuracy: 0.6060 - val_loss: 1.6541 - val_accuracy: 0.1646\n",
            "Epoch 371/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4674 - accuracy: 0.5944 - val_loss: 1.6504 - val_accuracy: 0.1732\n",
            "Epoch 372/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4421 - accuracy: 0.6067 - val_loss: 1.6365 - val_accuracy: 0.1744\n",
            "Epoch 373/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4798 - accuracy: 0.5913 - val_loss: 1.6273 - val_accuracy: 0.1757\n",
            "Epoch 374/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4602 - accuracy: 0.5951 - val_loss: 1.6331 - val_accuracy: 0.1794\n",
            "Epoch 375/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4659 - accuracy: 0.5972 - val_loss: 1.6325 - val_accuracy: 0.1658\n",
            "Epoch 376/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4647 - accuracy: 0.6086 - val_loss: 1.6400 - val_accuracy: 0.1744\n",
            "Epoch 377/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4277 - accuracy: 0.6204 - val_loss: 1.6401 - val_accuracy: 0.1769\n",
            "Epoch 378/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4510 - accuracy: 0.6065 - val_loss: 1.6439 - val_accuracy: 0.1634\n",
            "Epoch 379/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.4559 - accuracy: 0.6217 - val_loss: 1.6582 - val_accuracy: 0.1757\n",
            "Epoch 380/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.4313 - accuracy: 0.6258 - val_loss: 1.6588 - val_accuracy: 0.1658\n",
            "Epoch 381/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4316 - accuracy: 0.6225 - val_loss: 1.6544 - val_accuracy: 0.1744\n",
            "Epoch 382/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4150 - accuracy: 0.6378 - val_loss: 1.6691 - val_accuracy: 0.1744\n",
            "Epoch 383/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4360 - accuracy: 0.6294 - val_loss: 1.6697 - val_accuracy: 0.1732\n",
            "Epoch 384/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4146 - accuracy: 0.6384 - val_loss: 1.6834 - val_accuracy: 0.1732\n",
            "Epoch 385/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4012 - accuracy: 0.6474 - val_loss: 1.6848 - val_accuracy: 0.1744\n",
            "Epoch 386/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4319 - accuracy: 0.6386 - val_loss: 1.6722 - val_accuracy: 0.1757\n",
            "Epoch 387/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4299 - accuracy: 0.6318 - val_loss: 1.6953 - val_accuracy: 0.1658\n",
            "Epoch 388/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4212 - accuracy: 0.6462 - val_loss: 1.7219 - val_accuracy: 0.1769\n",
            "Epoch 389/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4333 - accuracy: 0.6360 - val_loss: 1.6861 - val_accuracy: 0.1683\n",
            "Epoch 390/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4185 - accuracy: 0.6412 - val_loss: 1.7127 - val_accuracy: 0.1695\n",
            "Epoch 391/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4198 - accuracy: 0.6344 - val_loss: 1.6927 - val_accuracy: 0.1794\n",
            "Epoch 392/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4549 - accuracy: 0.6290 - val_loss: 1.7294 - val_accuracy: 0.1708\n",
            "Epoch 393/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4300 - accuracy: 0.6360 - val_loss: 1.7231 - val_accuracy: 0.1708\n",
            "Epoch 394/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4058 - accuracy: 0.6487 - val_loss: 1.7233 - val_accuracy: 0.1671\n",
            "Epoch 395/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4178 - accuracy: 0.6421 - val_loss: 1.7129 - val_accuracy: 0.1720\n",
            "Epoch 396/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3901 - accuracy: 0.6567 - val_loss: 1.7225 - val_accuracy: 0.1671\n",
            "Epoch 397/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4258 - accuracy: 0.6431 - val_loss: 1.7175 - val_accuracy: 0.1695\n",
            "Epoch 398/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4206 - accuracy: 0.6409 - val_loss: 1.7247 - val_accuracy: 0.1671\n",
            "Epoch 399/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4288 - accuracy: 0.6314 - val_loss: 1.7146 - val_accuracy: 0.1769\n",
            "Epoch 400/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.4302 - accuracy: 0.6292 - val_loss: 1.7223 - val_accuracy: 0.1757\n",
            "Epoch 401/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4143 - accuracy: 0.6560 - val_loss: 1.7277 - val_accuracy: 0.1769\n",
            "Epoch 402/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3931 - accuracy: 0.6493 - val_loss: 1.7380 - val_accuracy: 0.1695\n",
            "Epoch 403/500\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.4169 - accuracy: 0.6424 - val_loss: 1.7337 - val_accuracy: 0.1757\n",
            "Epoch 404/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4136 - accuracy: 0.6485 - val_loss: 1.7335 - val_accuracy: 0.1708\n",
            "Epoch 405/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4306 - accuracy: 0.6391 - val_loss: 1.7404 - val_accuracy: 0.1757\n",
            "Epoch 406/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4226 - accuracy: 0.6486 - val_loss: 1.7267 - val_accuracy: 0.1695\n",
            "Epoch 407/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4240 - accuracy: 0.6440 - val_loss: 1.7274 - val_accuracy: 0.1806\n",
            "Epoch 408/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4150 - accuracy: 0.6415 - val_loss: 1.7396 - val_accuracy: 0.1720\n",
            "Epoch 409/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4039 - accuracy: 0.6489 - val_loss: 1.7443 - val_accuracy: 0.1658\n",
            "Epoch 410/500\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4210 - accuracy: 0.6400 - val_loss: 1.7798 - val_accuracy: 0.1744\n",
            "Epoch 411/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4298 - accuracy: 0.6220 - val_loss: 1.7170 - val_accuracy: 0.1683\n",
            "Epoch 412/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.4002 - accuracy: 0.6466 - val_loss: 1.7456 - val_accuracy: 0.1671\n",
            "Epoch 413/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4035 - accuracy: 0.6488 - val_loss: 1.7486 - val_accuracy: 0.1794\n",
            "Epoch 414/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.4226 - accuracy: 0.6466 - val_loss: 1.7494 - val_accuracy: 0.1695\n",
            "Epoch 415/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4264 - accuracy: 0.6341 - val_loss: 1.7608 - val_accuracy: 0.1646\n",
            "Epoch 416/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.4023 - accuracy: 0.6530 - val_loss: 1.7695 - val_accuracy: 0.1695\n",
            "Epoch 417/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.3986 - accuracy: 0.6471 - val_loss: 1.7596 - val_accuracy: 0.1658\n",
            "Epoch 418/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.4194 - accuracy: 0.6548 - val_loss: 1.7626 - val_accuracy: 0.1720\n",
            "Epoch 419/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3915 - accuracy: 0.6645 - val_loss: 1.7528 - val_accuracy: 0.1646\n",
            "Epoch 420/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3853 - accuracy: 0.6704 - val_loss: 1.7706 - val_accuracy: 0.1794\n",
            "Epoch 421/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3971 - accuracy: 0.6592 - val_loss: 1.7723 - val_accuracy: 0.1708\n",
            "Epoch 422/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.4001 - accuracy: 0.6732 - val_loss: 1.7807 - val_accuracy: 0.1744\n",
            "Epoch 423/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3924 - accuracy: 0.6660 - val_loss: 1.7840 - val_accuracy: 0.1781\n",
            "Epoch 424/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3768 - accuracy: 0.6689 - val_loss: 1.7838 - val_accuracy: 0.1769\n",
            "Epoch 425/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3737 - accuracy: 0.6877 - val_loss: 1.7900 - val_accuracy: 0.1720\n",
            "Epoch 426/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3813 - accuracy: 0.6751 - val_loss: 1.8034 - val_accuracy: 0.1708\n",
            "Epoch 427/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.3664 - accuracy: 0.6902 - val_loss: 1.8057 - val_accuracy: 0.1646\n",
            "Epoch 428/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3824 - accuracy: 0.6851 - val_loss: 1.8150 - val_accuracy: 0.1806\n",
            "Epoch 429/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3741 - accuracy: 0.6833 - val_loss: 1.8066 - val_accuracy: 0.1732\n",
            "Epoch 430/500\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.3772 - accuracy: 0.6903 - val_loss: 1.7959 - val_accuracy: 0.1658\n",
            "Epoch 431/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3738 - accuracy: 0.6868 - val_loss: 1.8190 - val_accuracy: 0.1646\n",
            "Epoch 432/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.3744 - accuracy: 0.6763 - val_loss: 1.8177 - val_accuracy: 0.1769\n",
            "Epoch 433/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3852 - accuracy: 0.6743 - val_loss: 1.8376 - val_accuracy: 0.1683\n",
            "Epoch 434/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3965 - accuracy: 0.6666 - val_loss: 1.8187 - val_accuracy: 0.1609\n",
            "Epoch 435/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3657 - accuracy: 0.6768 - val_loss: 1.8279 - val_accuracy: 0.1671\n",
            "Epoch 436/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3628 - accuracy: 0.6882 - val_loss: 1.8336 - val_accuracy: 0.1634\n",
            "Epoch 437/500\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.3659 - accuracy: 0.6843 - val_loss: 1.8199 - val_accuracy: 0.1744\n",
            "Epoch 438/500\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.3569 - accuracy: 0.6969 - val_loss: 1.8446 - val_accuracy: 0.1708\n",
            "Epoch 439/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3634 - accuracy: 0.6866 - val_loss: 1.8297 - val_accuracy: 0.1658\n",
            "Epoch 440/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3682 - accuracy: 0.6885 - val_loss: 1.8238 - val_accuracy: 0.1622\n",
            "Epoch 441/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3852 - accuracy: 0.6773 - val_loss: 1.8543 - val_accuracy: 0.1683\n",
            "Epoch 442/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3742 - accuracy: 0.6818 - val_loss: 1.8406 - val_accuracy: 0.1622\n",
            "Epoch 443/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3740 - accuracy: 0.6855 - val_loss: 1.8490 - val_accuracy: 0.1658\n",
            "Epoch 444/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3910 - accuracy: 0.6776 - val_loss: 1.8319 - val_accuracy: 0.1683\n",
            "Epoch 445/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3828 - accuracy: 0.6790 - val_loss: 1.8298 - val_accuracy: 0.1806\n",
            "Epoch 446/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3867 - accuracy: 0.6776 - val_loss: 1.8350 - val_accuracy: 0.1671\n",
            "Epoch 447/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3666 - accuracy: 0.6839 - val_loss: 1.8435 - val_accuracy: 0.1744\n",
            "Epoch 448/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3725 - accuracy: 0.6886 - val_loss: 1.8270 - val_accuracy: 0.1683\n",
            "Epoch 449/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3706 - accuracy: 0.6839 - val_loss: 1.8433 - val_accuracy: 0.1695\n",
            "Epoch 450/500\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.3790 - accuracy: 0.6763 - val_loss: 1.8328 - val_accuracy: 0.1695\n",
            "Epoch 451/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3678 - accuracy: 0.6895 - val_loss: 1.8628 - val_accuracy: 0.1671\n",
            "Epoch 452/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3760 - accuracy: 0.6887 - val_loss: 1.8386 - val_accuracy: 0.1683\n",
            "Epoch 453/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.3499 - accuracy: 0.7022 - val_loss: 1.8586 - val_accuracy: 0.1708\n",
            "Epoch 454/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3725 - accuracy: 0.6888 - val_loss: 1.8489 - val_accuracy: 0.1695\n",
            "Epoch 455/500\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.3661 - accuracy: 0.6956 - val_loss: 1.8724 - val_accuracy: 0.1658\n",
            "Epoch 456/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3572 - accuracy: 0.7058 - val_loss: 1.8582 - val_accuracy: 0.1744\n",
            "Epoch 457/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3560 - accuracy: 0.7018 - val_loss: 1.8640 - val_accuracy: 0.1695\n",
            "Epoch 458/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3499 - accuracy: 0.7057 - val_loss: 1.8755 - val_accuracy: 0.1671\n",
            "Epoch 459/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3415 - accuracy: 0.7157 - val_loss: 1.8737 - val_accuracy: 0.1720\n",
            "Epoch 460/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3713 - accuracy: 0.7000 - val_loss: 1.8947 - val_accuracy: 0.1708\n",
            "Epoch 461/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3579 - accuracy: 0.7012 - val_loss: 1.8854 - val_accuracy: 0.1708\n",
            "Epoch 462/500\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.3595 - accuracy: 0.7060 - val_loss: 1.8786 - val_accuracy: 0.1732\n",
            "Epoch 463/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3552 - accuracy: 0.6954 - val_loss: 1.8757 - val_accuracy: 0.1658\n",
            "Epoch 464/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3774 - accuracy: 0.6886 - val_loss: 1.8770 - val_accuracy: 0.1671\n",
            "Epoch 465/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3641 - accuracy: 0.6928 - val_loss: 1.8976 - val_accuracy: 0.1609\n",
            "Epoch 466/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3603 - accuracy: 0.7043 - val_loss: 1.8915 - val_accuracy: 0.1634\n",
            "Epoch 467/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3787 - accuracy: 0.6898 - val_loss: 1.9192 - val_accuracy: 0.1781\n",
            "Epoch 468/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3415 - accuracy: 0.6997 - val_loss: 1.8685 - val_accuracy: 0.1658\n",
            "Epoch 469/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3415 - accuracy: 0.7114 - val_loss: 1.9065 - val_accuracy: 0.1658\n",
            "Epoch 470/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3454 - accuracy: 0.7038 - val_loss: 1.8735 - val_accuracy: 0.1744\n",
            "Epoch 471/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3482 - accuracy: 0.7005 - val_loss: 1.9146 - val_accuracy: 0.1671\n",
            "Epoch 472/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3671 - accuracy: 0.7038 - val_loss: 1.9226 - val_accuracy: 0.1708\n",
            "Epoch 473/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3458 - accuracy: 0.7039 - val_loss: 1.8950 - val_accuracy: 0.1671\n",
            "Epoch 474/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3442 - accuracy: 0.7031 - val_loss: 1.9229 - val_accuracy: 0.1646\n",
            "Epoch 475/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3483 - accuracy: 0.7100 - val_loss: 1.8898 - val_accuracy: 0.1781\n",
            "Epoch 476/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3382 - accuracy: 0.7006 - val_loss: 1.9022 - val_accuracy: 0.1708\n",
            "Epoch 477/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3717 - accuracy: 0.6918 - val_loss: 1.9130 - val_accuracy: 0.1757\n",
            "Epoch 478/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3429 - accuracy: 0.7110 - val_loss: 1.8969 - val_accuracy: 0.1720\n",
            "Epoch 479/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3361 - accuracy: 0.7155 - val_loss: 1.9160 - val_accuracy: 0.1695\n",
            "Epoch 480/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3453 - accuracy: 0.7065 - val_loss: 1.9288 - val_accuracy: 0.1658\n",
            "Epoch 481/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3541 - accuracy: 0.7140 - val_loss: 1.9099 - val_accuracy: 0.1695\n",
            "Epoch 482/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3337 - accuracy: 0.7202 - val_loss: 1.9427 - val_accuracy: 0.1671\n",
            "Epoch 483/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3291 - accuracy: 0.7297 - val_loss: 1.9049 - val_accuracy: 0.1683\n",
            "Epoch 484/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3407 - accuracy: 0.7198 - val_loss: 1.9397 - val_accuracy: 0.1695\n",
            "Epoch 485/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3489 - accuracy: 0.7209 - val_loss: 1.9370 - val_accuracy: 0.1695\n",
            "Epoch 486/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3373 - accuracy: 0.7185 - val_loss: 1.9316 - val_accuracy: 0.1683\n",
            "Epoch 487/500\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.3340 - accuracy: 0.7198 - val_loss: 1.9439 - val_accuracy: 0.1769\n",
            "Epoch 488/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3418 - accuracy: 0.7231 - val_loss: 1.9299 - val_accuracy: 0.1683\n",
            "Epoch 489/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3391 - accuracy: 0.7236 - val_loss: 1.9563 - val_accuracy: 0.1671\n",
            "Epoch 490/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3280 - accuracy: 0.7241 - val_loss: 1.9463 - val_accuracy: 0.1708\n",
            "Epoch 491/500\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.3270 - accuracy: 0.7222 - val_loss: 1.9633 - val_accuracy: 0.1695\n",
            "Epoch 492/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3359 - accuracy: 0.7235 - val_loss: 1.9485 - val_accuracy: 0.1744\n",
            "Epoch 493/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3418 - accuracy: 0.7215 - val_loss: 1.9290 - val_accuracy: 0.1720\n",
            "Epoch 494/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3359 - accuracy: 0.7180 - val_loss: 1.9531 - val_accuracy: 0.1658\n",
            "Epoch 495/500\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3367 - accuracy: 0.7191 - val_loss: 1.9965 - val_accuracy: 0.1695\n",
            "Epoch 496/500\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.3272 - accuracy: 0.7265 - val_loss: 1.9494 - val_accuracy: 0.1695\n",
            "Epoch 497/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3319 - accuracy: 0.7281 - val_loss: 1.9697 - val_accuracy: 0.1658\n",
            "Epoch 498/500\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.3315 - accuracy: 0.7264 - val_loss: 1.9628 - val_accuracy: 0.1695\n",
            "Epoch 499/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3204 - accuracy: 0.7295 - val_loss: 1.9778 - val_accuracy: 0.1634\n",
            "Epoch 500/500\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.3368 - accuracy: 0.7222 - val_loss: 1.9710 - val_accuracy: 0.1720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s_stacked_embed/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: s2s_stacked_embed/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "tGU4pY52qkky",
        "outputId": "4472cc5c-0c9d-4aaa-c40f-ca1752b3e9b9"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Train Accuracy 0.7306046485900879\n",
            "Max Validation Accuracy 0.22727273404598236\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV1f3A8c/3Zg+yw8pkT5ERGQIiqAiIotW696B1VVtbrfqrtlZbq63bqlQRtYp1oCLiQByACLJnGIFAIAQyIYvMe35/nItJICEhJLnJzff9et3Xvc8557n3+4TwvU/Oc55zxBiDUkopz+VwdwBKKaWalyZ6pZTycJrolVLKw2miV0opD6eJXimlPJy3uwOoTVRUlElMTHR3GEop1WasWrUq2xgTXVtdq0z0iYmJrFy50t1hKKVUmyEiu+uq064bpZTycJrolVLKw2miV0opD6eJXimlPJwmeqWU8nD1JnoRiRORb0Vks4hsEpG7amkjIvKciKSIyHoRGVqt7joR2e56XNfUB6CUUur4GjK8sgK4xxizWkQ6AKtEZIExZnO1NpOBXq7HCOAlYISIRAAPA0mAce071xiT16RHoZRSqk71ntEbYzKMMatdrwuAZCDmqGbTgDeNtQwIE5EuwLnAAmNMriu5LwAmNekRKKVUW5W5BVIXNfvHnNANUyKSCAwBlh9VFQPsqba911VWV3lt7z0dmA4QHx9/ImEppVTb9O8R9vnPh5r1Yxp8MVZEgoEPgbuNMflNHYgxZoYxJskYkxQdXetdvEop5ZkqKyB3JzTTQlANSvQi4oNN8m8bY+bU0iQdiKu2Hesqq6tcKaXUEQsegueGwBd/bJa3b8ioGwFeA5KNMU/V0WwucK1r9M1I4JAxJgP4EpgoIuEiEg5MdJUppVT7ZAwsf8WewR+x7EX7vPFDKC1s8o9sSB/9aOAaYIOIrHWVPQDEAxhjXgbmA1OAFKAYuMFVlysifwVWuPZ7xBiT23ThK6VUG5KfAW+cDznbYUmXmnXDroezHga/4Cb/2HoTvTFmCSD1tDHA7XXUzQRmNio6pZRqy5xOMJXg5QPlh+H7f9gkD1CQUdUuJBbOf7bZwmiV0xQrpZRH+PhWWP8u9L8Qdn4HJQdtucMHnOX29cTHoP8FzRqGToGglFLNIW25TfIAmz+uSvIAv91U9fr0OyCseYeU6xm9Uko1Rn4GOCsgLA7WvgMpC8EnwHbBOCtg5sSa7UfeBnuWQ9KN0KETxA6HxNEtEqomeqWUaoyn+trnu9bZLpojEk6HzZ8c237S32tu37yg+WI7iiZ6pZQ6UdVvbHr21Jp11ZN+K6F99Eop1RA7voHCTPt69uX1t/cPgztWwaDL4fLZzRtbPfSMXiml6nM4D966CAKj4NdLYNsXdbft0AUSRttx8VE94RevtFiYddFEr5RS9UmeZ5+Ls+GdX9rX4d0gL7WqzZ8PwdrZEBwNPc9u+RiPQxO9UkrVxllp+9s7DYQFf7JliWNh12IIjYNpL8KsKTX3GXxFy8fZAJrolVKquqytsOUziBsO6/8H/M+WX/gyDLrUDpGM6g2VZbY89jQ4r65pwFoHTfRKqfZpzwrYtwZGTK9ZPvsKyN1Rs2zKP6vO1hNOryq/NxUCwkGOO0uM22miV0q1L8W5sH0BzP8DlB6Cz/8A92yFgv02YR+d5K+bB93G1v5egRHNH28T0ESvlPJ8mVtg4wcw7o/w6W8g+dOa9Uufhx9fOHa/az+pO8m3IZrolVKe771rIHsbLHqyZnlkL8hJOTbJO7zhsreh+5ktFWGz0kSvlPJsmVtskq/N2HvshGI/vgid+tsvgomPwrAbmmVeeHfRRK+U8kwrX4fVb4BxVpX1PMfOIjnqdjs/fP9p4BtYNbnYuPvs3PEeRhO9UsrzJM+DeXdXbfeZApe+BeIAx3FmfvHAJA86141SytNUltskH5ZgpyMAe+bu5X38JO/B9IxeKeU5inJg8T+hKAuueBc69oMFD0Hvc90dmVtpoldKtR27l9pFPi54/tiblHJT4dWzoDgHuo+HXhPB4QWXvumeWFuRehO9iMwEpgKZxpiBtdT/Abiq2vv1A6KNMbkisgsoACqBCmNMUlMFrpRqh16fbJ/PfQz8Q+14+KCOEBQFb06DilK4eg50G2eTvAIadkY/C3gBqPVr0RjzJPAkgIicD/zWGJNbrcl4Y0z2ScaplFJVirIhfx/87+qa5Ve+Bz3Pck9MrVi9id4Ys0hEEhv4flcA7p1hXynl+Qr2w+J/1Szr0LXd98XXpckuQYtIIDAJ+LBasQG+EpFVIjK99j1/3n+6iKwUkZVZWVlNFZZSqq0zxo55Ly2oKps1BXYshNF3wwP7bJ/8lCfcF2Mr15QXY88Hfjiq22aMMSZdRDoCC0RkizFmUW07G2NmADMAkpKSTG1tlFLtSPoq+PrPdhKyomyIGVqz/ty/wfBf2WGT137slhDbiqYcVHo5R3XbGGPSXc+ZwEfA8Cb8PKWUJ0hdDO9cBrk77dl7UbZN7p/fB6mL4MBGKNwPW+fDiF9X7Tfi1zbJq3o1yU9JREKBccDV1cqCAIcxpsD1eiLwSFN8nlLKA8ycDP3Oh/3r7Rqsta3Det6/oMcE+OAmKCuC0XfB8pdtnY6qabCGDK+cDZwJRInIXuBhwAfAGOP6iXMR8JUxpqjarp2Aj8SOdfUG3jHGHGdFXaVUu1GcC2lL7aMug6+G0262r6d/W7Ou+5nNFZlHasiom3oXQTTGzMIOw6xethM4tbGBKaU82L7VNbd7nQujboO83TD4Srsua8Lo2vf9vyw7Z41qMO3gUkq1vPQ1NbeLc2qepfeYUPe+3r7NEZFH00SvlGo5leWwataxY+BPv9Mt4bQXmuiVUi1n+Svw1YP2ddwICIqGaS9CQJh74/Jw2tGllGoahVl2eGRdUhdVJXmwffCXv61JvgXoGb1S6uSteBU+uwd+8R8YdGnNutICWP8efPFHu33Vh1BeBD3Pbvk42ylN9Eqpxvn0btvn3nUwzP+9LZtzC+z+Aab8s2q1ph9fhO/+bl9P+BP00gTf0jTRK6UaZ9Xr9nntf48qnwUJY2DQLyFjHax+EzoOgCFXwZBrWjxMpYleKXWijIH3rq1ZdvdGWPEf+OFZ8AuFpc9B6new5r/g8IELntOuGjfSRK+UarhlL0FFCSTPtdvDp9tHWBxMeMjOJrnmLbt83/710HmQXeEpopt7427nNNErpWqXuhhK86HveZC2DLK3VV1QPWLCn8A/xL728obACDujZPZ2u8pTv/PBx7/lY1c1aKJXStXujan2uctgyFh7bP2D+8En4NhyH3+Y9kLzxqZOiCZ6pVSVwwfhtYkQM6yqrHqS9wuxNzhlbak9yatWSRO9UqpK+krI3mofR5z5AIy7F36aAb3OgYjuwAVuC1GdOE30SrU3xti53f2Ca5ZnrIeFf63avuB56HOe7XcXgRG/atk4VZPRRK9Ue7PqdbtE3++2gG+gLdv4IXxwY1WbP6aBf6hbwlNNT+e6Uaq92TIfSg5B7g67/d+LbZIP6mi3Ow/SJO9hNNEr1R5snguvn2e7bHa7VnXK2QEVpZDytd2OHwn3p8MNn7svTtUstOtGqfZg63zYvQS+fNBOKAZ20e3d1ZbyCwg7tt9eeQRN9Ep5ssN58MUDsG623T4yP41fKCx6smbbzoNaNjbVYjTRK+WJdi0BBNa+A+vesWUOH3CW2wnHOvW3wyWH/wrO+YsdF99Zl3j2VJrolfI0xsCs86q2E8faxbanPgVRve2drsYJHfvDoMvsjU9dh7gvXtXs6k30IjITmApkGmMG1lJ/JvAJkOoqmmOMecRVNwl4FvACXjXGPN5EcSulAD6+HYKiYMBFUF4MAREw7+6abUbeZicWCwi34+GPSLqhZWNVbtOQM/pZwAvAm8dps9gYM7V6gYh4AS8C5wB7gRUiMtcYs7mRsSqlqqssr5oL/odn6m4XM8ze9KTarXoTvTFmkYgkNuK9hwMpxpidACLyLjAN0ESv1MmqKIM365iGICga7lwFO76FbV9Ah04tG5tqdZqqj36UiKwD9gG/N8ZsAmKAPdXa7AVG1PUGIjIdmA4QHx/fRGEp5aG2fQ5pP9Ysu205lByEyJ72hqcBF9qHaveaItGvBhKMMYUiMgX4GOh1om9ijJkBzABISko6zlLySrVzWz8/doWncX+Ejn3dE49q9U76zlhjTL4xptD1ej7gIyJRQDoQV61prKtMKdVYa2fD7Mvt67G/h+s/sxdZT73MvXGpVu2kz+hFpDNwwBhjRGQ49ssjBzgI9BKRbtgEfzlw5cl+Xp2MgXcug75TYNj1zfYxSrWoknxweIFPICz8Cyx52pZPfAxOv8O+vm+X28JTbUNDhlfOBs4EokRkL/Aw4ANgjHkZuAS4VUQqgMPA5cYYA1SIyB3Al9jhlTNdfffNQwT2LIfwhGb7CKVa3L9H2pOYARfBshdh6LUw+Uldnk+dkIaMurminvoXsMMva6ubD8xvXGiNEBBub/lWqi0pLYT170J0P0gcXVWeuQXyXb2dy16EpBvhvKdqjoVXqgE8685YTfSqrcnZAc8Prdoe/3+wZxmExsKqWTXbjn9Qk7xqFE30SrlLcS4sfd6+Dk+EvF3w7aNV9QHhdo3Wq+fY+eODotwRpfIAnpfo81Lrb6eUOxVlw1sXwf71dnvINTDtBfjuH5C+CrZ/act/nwJenvVfVLmHx/wWVToNi9PKGXU4Bz93B6NUXQr2w6vnwKE0u91lMEx+wr4+8z77/JdwiBupSV41GY/5TfJyCNsLfDjDFIDTCQ5dPEu1As5Ku9hHeCJkbgJxVCV5gBu/PHYEzR/3gJdPi4apPJvHJHoAr6BwHIVOKM23q+Uo5S7G2Aun6atg+UvH1k99BnpMqH2YpK7ypJqYRyV6vw6RUAgcztVEr1peWREcTLNdM2UFMOAXkLHu2HYxw3SKYNWiPCrRB4dGQgaUF+fjo7OyqpZ0ZJhkWIJN8gCb5tjnwVdB0k1QlGmnL4gd7r44VbvkUYk+NMxm99zcbDrFujkY1T4cTIMDm2Ht267t3dChKxTss9tXvg+9J1a1v3M1dOjS8nGqds2jEn1AsO2uOVxw0M2RqHbhwGZ4adSx5UOuttMF/zQDEk6vWRfZo2ViU6oaj0r0fsGhAJQVH3JzJKpd2LPMPk/4k50DvmN/eyG1QxdwVsAZ9+qFVdUqeFSiP3JGX3FYE71qJntWwDuX2gv+AP5hMPaeY6cm8PKB4OiWj0+pWnjUYPPA4HAAKg/nuzkS5ZEyk2smeYCuQ3T+GdXqedQZfXBwByqMA2dJgbtDUZ5mx7fw4U12LqVx90FpgT1rH6bDJFXr51mJPsCHQgLsf0KlTlbWVpvYt34OPzxjy2KHw/gH3BuXUifIoxK9j5eDIgKQMk306gStnW1vbpr8OGSsh7RlsOQpKMio2S42yT3xKXUSPCrRAxyWQBxlhe4OQ7UVTqcdCvnxr+32WX+CV8+CyjK7fcql0Ptcm+ALDkDnU9wXq1KN5HGJvsQRiE+5ntGreuxdCcGdYMP7di3WI/7Wter1pMdh5K1V2+GJLRaeUk3J4xJ9oXcYcRUZ9TdU7VdluT1rP54H9oFvUMvEo1Qz87hEn+fbhcGFa6pmD1QKbBfNundg3btV3TLVXTsX9m+Abx+D3pM0ySuP4nGJviAgFv/CUijKguCO7g5HtRZfP2SX7fMJgvKimnV3rISoXtB9HIy8TU8QlMep94YpEZkpIpkisrGO+qtEZL2IbBCRpSJyarW6Xa7ytSKysikDr0tJsGs2s7zdLfFxqjWqrLCzSRoDZcWQvw+WvwKDLoNLXqvZ9pLXbZI/wuHQRK88TkPO6GcBLwBv1lGfCowzxuSJyGRgBjCiWv14Y0z2SUV5AipDEgBwZm3BEXdaS32sag02zoGweFg3G1a8CkHR9i+7kBjbXTPuPjsPTZ/z4Izf28W5e9bTV6+UB6g30RtjFolI4nHql1bbXAa4dYLgyshe7HZ2JGbFTBwDLtJJpTxdZTns+AYSx8AHR92lWpRl71zd8AEkjq2aOfKKd1o+TqXcqKn76G8CPq+2bYCvRMQArxhjZtS1o4hMB6YDxMfHNzqAsCB/Xqi8kCczZsBzQ6DXREgYBdF9ITDSDqnzDWz0+6tWpLQQNrwH834LPc+uWXf+cxDdB+JHwjl/sWu1KtVONVmiF5Hx2EQ/plrxGGNMuoh0BBaIyBZjzKLa9nd9CcwASEpKMo2NIyzAh/crz2T6RZPolTITtn4Ga/9bLVAHeAdARHeISIQ+U6D/hZr825KNc+w4+OUvgXHaspSv7XP/aXD6b2reweof2vIxKtWKNEmiF5FBwKvAZGNMzpFyY0y66zlTRD4ChgO1JvqmEh7kA8C+kEH0uvxtO6wue5tdCagoC7KS7R2OxTn2NvfkT2HlTLjyPQjU9QdbvYrSY7toALx84dy/wfBbWj4mpVq5k070IhIPzAGuMcZsq1YeBDiMMQWu1xOBR0728+oTGuALQF6Ra6y0wwEd+9rH0SrKYNNHMPdOmHkuXD0HwuKaO0TVGFs+g3m/g0urjQm4/jP4+s9wyi9hyDX6V5lSdag30YvIbOBMIEpE9gIPAz4AxpiXgYeASODfYoelVRhjkoBOwEeuMm/gHWPMF81wDDXEhgfg7RC2HWjANAjevnDqZRAaC7OvgFlT4OaFOv6+Nfr0LvsX2UzX+quT/mEvwN78tXvjUqoNaMiomyvqqb8ZuLmW8p3Aqcfu0bz8fbzo07kDG9JPYJWpxNFw7Ufw+hR7dn/FuzqWujX54Tmb5Ks75RL3xKJUG+SRQxEGxYayds9BSsorG75TzDA462HY9gU8NxgK9jdfgKph9m+ArV/Y6YJjkuDWH6vqAiPdF5dSbYxHJvoLTo2hoKSCBz7aQMahww3fccSvofdkyNsFa95qtvjUcaSvsmPjD2yGl8fA7Mvs4h9nPwyd+sMfdsCvFulfXEqdAI+b6wZgVI9Ifj2uBy9/v4M5q9NJiAykT6cO9O0SQp9OHejTOZjEyCC8vY76nnM44Mp3YeYk+OZRe9aYdKN7DqI92r8R/jMB4kbCnmW2LCAC+k6BbmfY7aAo+1BKNZgY0+gh680mKSnJrFx58lPjpGYX8cXG/WxMP0RyRj67copwug7X18tB9+ggRveM4pJhsfTrElK1Y95u21ef+j1MfgKGT9czyOa283t46yIw1brbxt4DE/6kP3ulGkBEVrkGwhxb58mJ/mgl5ZWkZBay7UABWw8UkJxRwNKUbCqN4eYx3bh3Ul98jpzlV5TCe9fBts/tDThn/xkcXk0eU7vmdNq1WBc9CeXFNetihsEt37gnLqXaoOMleo/suqmLv48XA2NCGRhTdadkTmEpTy3Yxn8Wp7LvYAnPXD7YJntvP7j8HZh/Dyx9DvatgV/MgJCux/kE1WAl+fCvvlVTBvc7HyY/ae90Xf2mHTqplGoS7SrR1yYy2I/HLjqFblFBPPpZMrERAdw/uZ+tdDjgvKfs2eX8e+HFkTDp7zDkKvcG3VYVZsH6dyGiB2z+uCrJD58OEx+z9zUAjL/ffTEq5YHafaI/4uax3UnJLOQ/i3YysX9nhiWE2woRGHK1XS901nnwyW32deJod4bbNn18K6QsqNp2+ICzHM74Q1WSV0o1OY8cXtlYD57Xjy6hAfzh/XUcLjtqDH7iGPh9ip3b/I3z4bvH3RNkW1FeYhf8AFj/HjzRwyb5bmfA6XfCb9bAQ9nwUK7eiaxUM9NEX00Hfx+evGQQO7OLeOLLLcc2CI6GXy2GgRfDd3+HWVNh44f2oqKy9qyAkkPw4U3wVD946xcw5xYozga/ENsVNvFRO3so6AVupVqAdt0c5fSeUVw3KoHXf9jFOf07cXqPo8ZsB0XCRS/bm3dWzoQPboTE1+GcRyBmqHuCdrfyElj8L9i9FHYvgbgRsGe5rdux0D5f/Br0mayLbivlBu1qeGVDHS6rZMpziymrcDL/N2MJDfSpvaHTCatnwcK/wuFc6NjfzoN+/nPta+z3wr/C4n/WXhcaB3dvaF8/D6XcQMfRN8LaPQf55ctLGdsrmlevTcLhOE6iKsmHZS/Bd3+z26PusOPuver4gmiLDufBzMlw2k12DprekyAvFZLnQVq11SSHXgt9p9q7in2D7IpeOs+/Us1OE30jvfnjLh76ZBNXjojnoan98feppz+5ogxmX267K0JiIPY0O1d6j/Ftt8uiotTeU7DpI3j/+rrbXfUBrH7DLv4R1vilIJVSjaM3TDXSNSMTSM87zCuLdpJXVMaLVw49/pm9ty9cMwc2z4VNc2DXEjtePLgTXPa27dZpC10YO76BlIWQsQ52LYah19kkfsQ1H8PS5+3drN7+drGWXufYh1Kq1dEz+gZ4+fsdPP75Fib07cizlw+mg38Du2SKc2HdbPjxRchPt90ZQ6+FU6+0Y/HFAV6t6Lv2wCY7JPLtOuZ67zsVxj9oL0QrpVoV7bo5ScYY/rtsN3/5dDP9uoQw64bTiAz2a/gbFGXbro/U7+2SeEcWtA5LgGkvQueBEBDePMHXpaLUPmadBwMutN1O3x91b0C/822btOXw68W2S6Yt/EWiVDukib6JfLPlALf+dzWdQ/2Zef1p9IgOPvE3yd1pu0VSF8G2L6Gy1J7ZdxsHI2+FXhObJ5mWFUFuKix52k7j8OII2/dekFGznW8w3PYjBEaBTwBUlkFpoR1WqpRqtTTRN6HVaXnc8sZKKpyGl68exqgeJ5EAs7fD909A9jZ71p+/1ybYsDi7jm1AOORn2PnXR91ub87qOMDesCUCzgooLYDkT+3Zd2kBdOgM5Yftxd/iHDuvfmW5nWPmCN9gKCs8Np5bf4TInjodgVJtkCb6JrYnt5gbZq1gd04Rf7voFH6ZFHfyb1p+GDZ8AGk/Qs4OSF9pE/nxePnZs/LS/IZ/Tvwo+xnVXfgyDD7u0sBKqVbupBO9iMwEpgKZxpiBtdQL8CwwBSgGrjfGrHbVXQf8n6vpo8aYN47e/2itPdEDHDpczu1vr2ZJSjY3ju7Gn6b2Q5q6y+XgHnsjVkC4HcFzKB1WzYLo3pC1zc4b46ywffyr34KuQ6DwAET3sV8cgZEw8BcQFA1lxfYvhz6T7fBPZ6Wtd1ZA/MimjVsp1eKaItGfARQCb9aR6KcAd2IT/QjgWWPMCBGJAFYCSYABVgHDjDF5x/u8tpDoAcornTz2WTKzlu7ilrHduH9yv+MPv1RKqWZy0uPojTGLRCTxOE2mYb8EDLBMRMJEpAtwJrDAGJPrCmQBMAmY3fDwWy8fLwcPn9+fSqfhP4tTWb/3EE9ecirxkYHuDk0ppX7WVLNXxgB7qm3vdZXVVX4MEZkuIitFZGVWVlYThdX8RIRHpg3giYsHsXlfPuc+s4h56/e5OyyllPpZq5mm2BgzwxiTZIxJio6Odnc4J0REuPS0OL787RkM6BrCb2av4Z3labTGC91KqfanqRJ9OlB96Emsq6yuco/UNSyAN28azuieUTzw0QZ+8+5a8kvK3R2WUqqda6pEPxe4VqyRwCFjTAbwJTBRRMJFJByY6CrzWIG+3sy6YTh/OLcP8zdkMPmZxXyy1mO/25RSbUCDLsaKyGzshdUoEdkLPAz4ABhjXgbmY0fcpGCHV97gqssVkb8CK1xv9ciRC7OezMsh3D6+J6N6RPLQJxu56921LNmezSPTBhLgqysqKaValt4w1cwqnYZnv97G89+m0LtjB/5xySAGx4W5OyyllIc53vDKVnMx1lN5OYTfTezDGzcMJ6+4jIv+/QP3z1lPblGZu0NTSrUTmuhbyBm9o1l4zzhuHtON91buZcK/vuPt5bupdLa+v6iUUp5FE30L6uDvw4Pn9efzu8bSt3MHHvxoI2c/9T0b0w+5OzSllAfTRO8GvTt1YPYtI3n56mEUlFQw9fklTHpmEat2H3dmCKWUahRN9G4iIkwa2JmvfnsGD03tT/7hci55eSmPzttMYWk9s1YqpdQJ0FE3rURBSTn/+GIL/12WRkSQL7ed2YOrRybUvyC5Ukqho27ahA7+Pjx64Sl8cvtoBnQN4dHPkjnzSXvBtrzS6e7wlFJtmCb6VubUuDDeumkEs28ZSUx4wM8XbOet36dz5yilGkW7bloxYwzfbs3kyS+3kZyRz4CuIUzs35lfJsXSNSzA3eEppVoRXUqwjat0Gt5fuYc3f9zN5ox8fL0dXDcqgdvO7El4kK7vqpTSRO9R9uYV88zX25mzei9Bvt5MP6M7N47pRpBfg6YtUkp5KE30HmjbgQL++eVWvtp8gKhgX24a051fDI2hU4i/u0NTSrmBJnoPtjotj39+uZWlO3Lw9XIwICaE4YkR3Da+J6EBPu4OTynVQjTRtwM7swp5Y+kuNu7LZ9XuPPx9HFx3eiK/PqOH9uMr1Q5oom9n1u45yKwfUvlk3T6CfL25cEhX7hjfi86h2q2jlKfSRN9ObTtQwEvf7eCzDRl4O4RrRiVw85juRHfwc3doSqkmpom+nUvLKeafX21l3vp9+Hg5GBwXhjFw19m9SEoMx89bp1lQqq3TRK8ASM0u4pXvd7AkJZu9eYcBiAkL4E9T+3F2v054e+mN0kq1VZro1TEKSytYsj2bf321le2ZhZwSE8qfL+jPsIQId4emlGoETfSqTmUVTj5dt4+/zU8mp6iM0T0j+c2EXozoHunu0JRSJ0ATvapXcVkF7yxP4+Xvd5JdWMqAriGM7B7JJcNi6dclxN3hKaXqcdKJXkQmAc8CXsCrxpjHj6p/Ghjv2gwEOhpjwlx1lcAGV12aMeaC+j5PE737lJRXMvunNL7YuJ81ew5SVuFkWEI4V42IZ8opXXR+fKVaqZNK9CLiBWwDzgH2AiuAK4wxm+tofycwxBhzo2u70BgTfCIBa6JvHQ4Wl/HBqr28vTyN1OwiwgJ9OKNXNNeMSiApIRwRcXeISimX4yX6hsyENRxIMcbsdL3Zu8A0oNZED1wBPNyYQFXrEhboy81ju3Pj6G78uDOHD1ftZeGWTOau20diZCCD48IYmhDOL4fFEeCrZ2trvU4AABJdSURBVPpKtVYNSfQxwJ5q23uBEbU1FJEEoBvwTbVifxFZCVQAjxtjPq5j3+nAdID4+PgGhKVaisMhjO4ZxeieURwuq2TOmr18tzWLpTty+HjtPv46bzPXjkpk0sDOzP4pjZ4dg7llbHd8dLimUq1CU89teznwgTGmslpZgjEmXUS6A9+IyAZjzI6jdzTGzABmgO26aeK4VBMJ8PXiqhEJXDUiAYBlO3N4a9luXluSymtLUn9u901yJi9cOVSnXVCqFWhIok8H4qptx7rKanM5cHv1AmNMuut5p4h8BwwBjkn0qm0a2T2Skd0j+c2EAlKzC+kcGsDunCLun7OBiU9/zy1ju3Pt6Yk6k6ZSbtSQi7He2IuxZ2ET/ArgSmPMpqPa9QW+ALoZ15uKSDhQbIwpFZEo4EdgWl0Xco/Qi7FtX0pmIY9/nszXyZkE+3lzeo9IhneL4JdJcZr0lWoGJ3Ux1hhTISJ3AF9ih1fONMZsEpFHgJXGmLmuppcD75qa3xz9gFdExIldiPzx+pK88gw9Owbz6nWnsWnfId5cupvlqTl8tfkAL3ybQmJkEImRgdw0pjunxIa6O1SlPJ7eMKVazKZ9h3jpux0cLC5n/d6D5JdUcE7/Tkwb3JWBXUNJiAzUIZtKNZLeGatanfyScl5bnMrby3eTXVgGQL8uIXQO8aNTiD+/O6c3HXVZRKUaTBO9arXKK50s3p7F7pxiPlm7j8NllaRmF+Hv4+AP5/Zh2pAYQvy1T1+p+miiV21KanYR936wjhW78vD1djAsPpyoDn6M7hHJoNgw+nXpoF08Sh1FE71qc4wxrNt7iI/XpLMkJZvswlIOFpcDkJQQzrWnJ3LeKV3wcmjCVwo00SsP4HQa0nKL+WZLJq8vTWVPrl04JTEykNBAX34xJIarRsTr4imq3dJErzyK02n4avN+NqQfIiWzkH0HS9iQfoiwQB/O6tuJs/p1pHOoPwO7huLrrYlftQ+a6JVHM8awMDmT+Rsy+Dr5APklFQAE+3lz4ZCujOkZTbeoIOIjAtmbV0xJuVPH7yuPo4letRvllU7W7TlIZkEpXycfYN76DMoqnMe0e2TaAK4ekYBD+/iVh9BEr9qtwtIKUrOKSM0pYtv+An7YkU1RaQXbDhTSwc+b03tGUlRayagekdx2Zg8dzaPaLE30SlXjdBo+Xb+PpSk5LEnJxtfbQWp2EQNjQpjQtxPXjkogKtjP3WEqdUI00St1HMYYnv56O/PW72NXdhE+Xg56dgwmJiyAQbGhXHt6ot60pVo9TfRKNdCOrEL+u2w3u7KL2JN3mJTMQuIiApjQpyNhgb5celocMWEB7g5TqWNooleqkZbtzOGpr7axcncuTgO+Xg6uHBHPiG4RBPt707dzCNEdtJtHuZ8meqVOUqXTsD+/hGe/3sYHq/bidP238fdxcPOY7tw8ththgb7uDVK1a5rolWpCBSXl7M4ppqCkgtk/pTF33T4AOnbwIzEyiHP6d2J4twh25RQR4u/DuN7ROoxTNTtN9Eo1o+SMfL7Zksmu7CI2pB9iy/6CGvWdQvyIjwjk/FO7Eh3sxzn9O+lUDarJndQKU0qp4+vXJYR+XUIAO4Jn64ECdmQWUVxWgbeXMGd1Ouv2HGTFLrv6Zv8uIVw5Ip7LT4urNeFXVDpxiOhfAarJ6Bm9Ui2gtKKSjen5bDtQwCvf72BXTjG9OwXzwJR+nNErmoz8Eg4Wl7F4ezb/WbSTU2JDefXaJD3zVw2mXTdKtSLGGBZsPsCjnyWTlltMoK8XxWWVP9f7eAnllYbrT0/kzxcMAGBnViFdQgMI8PVyV9iqldOuG6VaERFh4oDOjOsTzRcb97NiVy4xYYF08PcmwMeLi4fF8ui8zby6JBWHCBFBPvxrwTYGxYbx76uG6jh+dcL0jF6pVqjSaXjok428vTwNAIeAAfy8Hdx1Vm8uHhpDVLCf9uOrn510142ITAKeBbyAV40xjx9Vfz3wJJDuKnrBGPOqq+464P9c5Y8aY96o7/M00StlbUw/RG5RGWN6RrHv0GEe/mQTC7dkAtAtKogh8WFsP1BIRJAvV46IZ2L/TjoxWzt1UoleRLyAbcA5wF5gBXCFMWZztTbXA0nGmDuO2jcCWAkkYU9IVgHDjDF5x/tMTfRK1c4YQ3JGAQuTD/DRmnTyS8rp1yWEnVlFpB88TGSQL3ERgVw9MoHzT+2Cn7f26bcXJ9tHPxxIMcbsdL3Zu8A0YPNx97LOBRYYY3Jd+y4AJgGzGxK4UqomEaF/1xD6dw3hzrN6/VxeVuHkozV7+WzDfrbtL+D376/jyS+3MOWULozv05ER3SN+TvrGGP7y6Waign25fXxP/QugHWhIoo8B9lTb3guMqKXdxSJyBvbs/7fGmD117BtT24eIyHRgOkB8fHwDwlJKHeHr7eCy0+K57LR4jDEs3ZHDa0tSeWd5Gq//sIvOIf6c0TuKtNxiUjILyS4sAyA1u5i/TBtAsJ+Oy/BkTfWv+ykw2xhTKiK/At4AJpzIGxhjZgAzwHbdNFFcSrU7IsLonlGM7hnF4bJKfkjJ5tUlO1mw+QBxEYGc1bcT/buGkFNUxvPfbOeHlGxuHJPI5IFdiIsIdHf4qhk0JNGnA3HVtmOpuugKgDEmp9rmq8AT1fY986h9vzvRIJVSjRPg68XZ/Ttxdv9OtdaP6x3NPz7fwt/mb+FfX23j+tGJnNErmmEJ4fj7ePHFxv3MXZfOLWO7MyQ+vIWjV02lIRdjvbHdMWdhE/cK4EpjzKZqbboYYzJcry8C7jPGjHRdjF0FDHU1XY29GJt7vM/Ui7FKtayUzAL+Pn8L32/LosJp6ODnzcCYUH7cWXUON7J7BKclRjCudzQ9OwbXOVunMYb8kgryispIjApqqUNo95pieOUU4Bns8MqZxpjHROQRYKUxZq6I/B24AKgAcoFbjTFbXPveCDzgeqvHjDGv1/d5muiVco/C0gqW78zh84372bI/n8FxYdw5oRcfr0nnw9V72ZlVRIXTEOTrxX2T+zJpQGc6hvgDsDotj9eWpLJ6dx4Zh0oQgacvHcyFQ2q9LKeamE6BoJRqEgeLy/huaxZv/LiLNWkHcQiM6hFJoK83CzYfAOyZ//DECL7YtJ+UzEKevmww0wZrsm9umuiVUk3KGMOW/QV8um4f327NIjO/hPNP7crNY7sRG24v6BaXVXDD6yv4aVcu903qy6/O6K5DOZuRJnqllFuUlFfy+/fXMW99BuP7RHP96G4MjgsjNEAXW29qOqmZUsot/H28eP6KIQyKDeXf3+3g25k/AdA5xJ/+XUMQwMshnDeoC707dSDQ14vVaXl8uyWLMb2iuDQprsb7ZRw6zL0frGdUj0huO7OnG46obdIzeqVUizhcVslPu3JJzshnY/ohUjILKS6rpLiskuzC0hptfb0dlFU46dcl5OdZPQ2wdX8+B/Jt2/F9ojlvUFcuHhqjXUJo141SqhVzOg2r0vLILijlQH4JfTqHkJQYzuOfb2HZzhwqnQYfLweVTkOl03DPxN7M/imNRduzqXQa/LwdXDwsloem9sffp/3O7aOJXinlcZxOwz+/2kpyRj7fbs1i0oDOvHDlkHa7Kpf20SulPI7DIdw7qS8Ary7eyaOfJXPWU98zolsEN47pRu+OHX6er3/bgQLiIwLx83a0y24eTfRKqTbv5rHdiQ0P4O3laXy0Jp33Vu4lPNCHKad0IeNQCd+45vD39XIQFxHAtMExJCWE4zTwdfIBkjPyuWF0IhFBfgyND/O4vwq060Yp5VEyDh1myfZsvti4n++2ZRHi783Z/ToRGuBD8v58fkjJqdHeIeDn7cXhcrtub4/oIO6Y0JOJ/TsT1IZm9dQ+eqWUcimrcLIh/RCl5ZUYIDEqiAAfLxYmH6CotIK3lu1mR1YR8RGBPHrhQAJ9vSirdDI0PpxKpyGroJRtBwro1akD3WqZy6e0otItC75ooldKqQaqdBoWbc/i/g83sD+/5Odyh4BDhApnVc6MCQtgWEI4vToGk1dczrKdOWzOyOf8U7vy12kD6pz4rTlooldKqRN06HA5P6Xm4uMllFca1u89SEl5JT07BhMbHkhyRj5r9hzkp9RcsgpK8XYIp8aFkRgZxNx16UQF+3H7+J5cOCSmRRZ20USvlFLNxBhDTlEZkUG+P4/oWb/3IHe/u5ad2UX4+zg4q18nSsoqGRQbxtRTu7BtfwFfbT7AkPgwJvbvTOdQf4pKK0jLLaZfl5BGxaGJXimlWlil07A6LY8PVu7l47XpGANllc5j2jkEYsMD2XfwMGGBvqx48KxGDQHVcfRKKdXCvBzCaYl2sZa/XjgQb4ewdu9B1qYdxGkMUwd1Jf1gMUu255CSVch5g7owqnskxkBTD/XXRK+UUs3M19uOyx8aH87Qaksydg71Z1hCRLN/vmfdFaCUUuoYmuiVUsrDaaJXSikPp4leKaU8XIMSvYhMEpGtIpIiIn+spf53IrJZRNaLyEIRSahWVykia12PuU0ZvFJKqfrVO+pGRLyAF4FzgL3AChGZa4zZXK3ZGiDJGFMsIrcCTwCXueoOG2MGN3HcSimlGqghZ/TDgRRjzE5jTBnwLjCtegNjzLfGmGLX5jIgtmnDVEop1VgNSfQxwJ5q23tdZXW5Cfi82ra/iKwUkWUicmFdO4nIdFe7lVlZWQ0ISymlVEM06Q1TInI1kASMq1acYIxJF5HuwDcissEYs+PofY0xM4AZrvfJEpHdjQwjCshu5L5tlR5z+6DH3D409pgT6qpoSKJPB+Kqbce6ymoQkbOBB4Fxxpifl3Q3xqS7nneKyHfAEOCYRF+dMSa6AXHVSkRW1jXfg6fSY24f9Jjbh+Y45oZ03awAeolINxHxBS4HaoyeEZEhwCvABcaYzGrl4SLi53odBYwGql/EVUop1czqPaM3xlSIyB3Al4AXMNMYs0lEHgFWGmPmAk8CwcD7rlnX0owxFwD9gFdExIn9Unn8qNE6SimlmlmD+uiNMfOB+UeVPVTt9dl17LcUOOVkAmyEGS38ea2BHnP7oMfcPjT5MbfK+eiVUko1HZ0CQSmlPJwmeqWU8nAek+jrm4+nrRKRmSKSKSIbq5VFiMgCEdnueg53lYuIPOf6GawXkaHui7zxRCRORL51zZ+0SUTucpV77HGLiL+I/CQi61zH/BdXeTcRWe46tv+5Rr4hIn6u7RRXfaI74z8ZIuIlImtEZJ5r26OPWUR2icgG1/xfK11lzfq77RGJvtp8PJOB/sAVItLfvVE1mVnApKPK/ggsNMb0Aha6tsEefy/XYzrwUgvF2NQqgHuMMf2BkcDtrn9PTz7uUmCCMeZUYDAwSURGAv8AnjbG9ATysHee43rOc5U/7WrXVt0FJFfbbg/HPN4YM7jaePnm/d02xrT5BzAK+LLa9v3A/e6OqwmPLxHYWG17K9DF9boLsNX1+hXgitrateUH8Al2Ur12cdxAILAaGIG9Q9LbVf7z7zl2uPMo12tvVztxd+yNONZYV2KbAMwDpB0c8y4g6qiyZv3d9ogzek58Pp62rpMxJsP1ej/QyfXa434Orj/PhwDL8fDjdnVhrAUygQXYO8gPGmMqXE2qH9fPx+yqPwREtmzETeIZ4F7A6dqOxPOP2QBficgqEZnuKmvW321dHLyNM8YYEfHIMbIiEgx8CNxtjMl33YwHeOZxG2MqgcEiEgZ8BPR1c0jNSkSmApnGmFUicqa742lBY4yd/6sjsEBEtlSvbI7fbU85o2/QfDwe5ICIdAFwPR+ZdsJjfg4i4oNN8m8bY+a4ij3+uAGMMQeBb7HdFmEicuSErPpx/XzMrvpQIKeFQz1Zo4ELRGQXdvrzCcCzePYxY6rm/8rEfqEPp5l/tz0l0dc7H4+HmQtc53p9HbYP+0j5ta4r9SOBQ9X+HGwzxJ66vwYkG2OeqlblscctItGuM3lEJAB7TSIZm/AvcTU7+piP/CwuAb4xrk7ctsIYc78xJtYYk4j9P/uNMeYqPPiYRSRIRDoceQ1MBDbS3L/b7r4w0YQXOKYA27D9mg+6O54mPK7ZQAZQju2fuwnbL7kQ2A58DUS42gp29NEOYAN21S+3H0MjjnkMth9zPbDW9ZjiyccNDMKu1Lbe9R//IVd5d+AnIAV4H/Bzlfu7tlNc9d3dfQwnefxnAvM8/Zhdx7bO9dh0JFc19++2ToGglFIezlO6bpRSStVBE71SSnk4TfRKKeXhNNErpZSH00SvlFIeThO9Ukp5OE30Sinl4f4fbFGjFGScTCEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}